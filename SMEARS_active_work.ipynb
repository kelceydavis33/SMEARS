{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "01d54b64",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c3d3a4a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "from astropy import units as u\n",
    "from astropy.coordinates import SkyCoord\n",
    "from astropy.coordinates import match_coordinates_sky\n",
    "from math import e\n",
    "import os\n",
    "import sys\n",
    "from scipy import io\n",
    "from astropy.time import Time\n",
    "from astropy import wcs\n",
    "import pandas as pd\n",
    "#import skimage\n",
    "import scipy as sp\n",
    "#from skimage.feature import register_translation \n",
    "from matplotlib import cm, colors\n",
    "from matplotlib.collections import PatchCollection\n",
    "import scipy.misc\n",
    "import pyradiosky\n",
    "from pyradiosky import SkyModel\n",
    "from astropy.coordinates import Longitude, Latitude\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f8a32f6",
   "metadata": {},
   "source": [
    "## Personal Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f9a95940",
   "metadata": {},
   "outputs": [],
   "source": [
    "gleam_path = '/home/kelcey/obs_files/GLEAM_v2_plus_rlb2019_with_labels.sav'\n",
    "old_sources = '/home/kelcey/obs_files/old source arrays/'\n",
    "new_sources = '/home/kelcey/obs_files/new source arrays/'\n",
    "\n",
    "directory2 = '/home/kelcey/source_arrays/old/'\n",
    "\n",
    "directory = '/home/kelcey/hera/Washington/uwashchamp2019/source_array_data/'\n",
    "GLEAM_path = '/home/kelcey/SeniorYear/GLEAM_v2_plus_rlb2019.sav'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06ad9956",
   "metadata": {},
   "source": [
    "## Generating the matched observation table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "addc3c65",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_df(df_path):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ---\n",
    "    df_path: string\n",
    "            absolute path to where a pandas DataFrame generated by the match_to_GLEAM function\n",
    "             has been generated\n",
    "    Returns\n",
    "    ---\n",
    "    table: DataFrame\n",
    "            the loaded pandas DataFrame\n",
    "    \"\"\"\n",
    "    table = pd.read_pickle(df_path)\n",
    "    return table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8871146b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_df(table, sav_path):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ---\n",
    "    table: DataFrame\n",
    "            a generated pandas DataFrame from the match_to_GLEAM function\n",
    "    sav_path: string \n",
    "            a string absolute path to a folder where the file should be saved\n",
    "    \n",
    "    Returns\n",
    "    ---\n",
    "    A .pickle file saved to the specefied directory\n",
    "    \"\"\"\n",
    "    table.to_pickle(sav_path + 'SMEARS_table.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c75fa099",
   "metadata": {},
   "outputs": [],
   "source": [
    "def match_to_gleam(directory, GLEAM_path, index_lim = None, missed_source_table = False, \n",
    "                   dict_ind_list = None, observation_freqs = True, flux_type = 'I'):\n",
    "    \"\"\"\n",
    "    Matches all the .sav files in a given directory with GLEAM files and creates a Pandas DataFrame from \n",
    "    the data\n",
    "    \n",
    "    Parameters\n",
    "    ---\n",
    "    \n",
    "    directory: string\n",
    "                an absolute path to the data files that have been proccessed through FHD\n",
    "    \n",
    "    GLEAM_path: string\n",
    "                an absolute path to a saved GLEAM .sav file\n",
    "    \n",
    "    index_lim:  integer, optional\n",
    "                index cut-off of the last observation you want processed. This is helpful in limiting\n",
    "                time code takes to run and exploring small chunks of your data\n",
    "                \n",
    "    missing_source_table: bool\n",
    "    \n",
    "    dict_ind_list: bool\n",
    "    \n",
    "    observation_freqs: string\n",
    "                \n",
    "    flux_type:  string\n",
    "                is 'I' by default, can be 'Q', 'U', or 'V'\n",
    "    \n",
    "    Returns\n",
    "    ---\n",
    "    df\n",
    "        A pandas DataFrame containing matching information for sources. Each row is an object in the input\n",
    "        GLEAM file. The columns represent the input fies matched to GLEAM. The names of the rows indicate \n",
    "        which data file is being matched, indicated by an integer index based on it's numerical order in the\n",
    "        input directory. The columns indicate the following information:\n",
    "        \n",
    "            RA:\n",
    "                original right asention coordinate value in decimal degrees specified in GLEAM file\n",
    "            \n",
    "            DEC:\n",
    "                original declination coordinate value in decimal degrees specified in GLEAM file\n",
    "            \n",
    "            Mag GLEAM:\n",
    "                original magnitude in Jansky of the object as specified in the GLEAM file\n",
    "            \n",
    "            RA EO Gleam:\n",
    "                right asention in degrees for extended components of the object as specified in the GLEAM\n",
    "                file. If no extended components are specified, this values is zero.\n",
    "            \n",
    "            DEC EO GLEAM\n",
    "                declination in degrees for extended components of the object as specified in the GLEAM\n",
    "                file. If no extended components are specified, this values is zero.\n",
    "            \n",
    "            Mag EO GLEAM\n",
    "                magnitude in Jansky for extended components of the object as specified in the GLEAM\n",
    "                file. If no extended components are specified, this values is zero.\n",
    "                \n",
    "            The remaining columns include the integer identifier of the data file being matched to gleam.\n",
    "            These columns have the following form:\n",
    "            \n",
    "            Mag {integer identifier}:\n",
    "                The point-like high-level value for the magnitude of the source in the data file that best\n",
    "                matched the GLEAM source specified by the row. If no match was made, a 0 is added in this\n",
    "                position.\n",
    "            \n",
    "            Distance {integer identifier}:\n",
    "                This is the matching distance in decimal degrees for the match indicated in the data. For\n",
    "                cases where no match was made, the closest matchin distance is added\n",
    "            \n",
    "            RA {integer identifier}:\n",
    "                The point-like high-level value for the right asention in degrees of the source in the data \n",
    "                file that best matched the GLEAM source specified by the row. If no match was made, a 0 \n",
    "                is added in this position.\n",
    "            \n",
    "            DEC {integer identifier}:\n",
    "                The point-like high-level value for the declination in degrees of the source in the data \n",
    "                file that best matched the GLEAM source specified by the row. If no match was made, a 0 \n",
    "                is added in this position.\n",
    "            \n",
    "            EO RA {integer identifier}:\n",
    "                The right ascention in decimal degrees of the low-level extended components of the object \n",
    "                in the data files being processed that best matched the GLEAM object specefied by the row. \n",
    "                If no match was made or no extended components are present, a 0 is added in this position.\n",
    "            \n",
    "            EO DEC {integer identifier}:\n",
    "                The declination in decimal degrees of the low-level extended components of the object in \n",
    "                the data files being processed that best matched the GLEAM object specefied by the row. If \n",
    "                no match was made or no extended components are present, a 0 is added in this position.\n",
    "            \n",
    "            EO MAG {integer identifier}:\n",
    "                The magnitude in Janksy of the low-level extended components of the object in \n",
    "                the data files being processed that best matched the GLEAM object specefied by the row. If \n",
    "                no match was made or no extended components are present, a 0 is added in this position.\n",
    "            \n",
    "            STON {index identifier}:\n",
    "                The signal to noise ratio of the high-level point-like source indicated by the data file \n",
    "                for the indicated best match source.\n",
    "            \n",
    "            STON EO {index identifier}:\n",
    "                The signal to noise ratio of the low-level extended points of the source indicated by the \n",
    "                data file for the indicated best match source.\n",
    "            \n",
    "    \"\"\"\n",
    "    observ_freqs = []\n",
    "    missing_dat = {}\n",
    "    #Create paths to all the .sav files in the specified directory\n",
    "    paths = glob.glob(directory + '*source_array.sav')\n",
    "    if dict_ind_list == None:\n",
    "        a = 0\n",
    "    else:\n",
    "        paths = np.array(paths)[dict_ind_list]\n",
    "    #Specify the matching distance\n",
    "    match_dist = 0.1\n",
    "    #Specify the fraction of flux of the GLEAM magnitude that a source must be to be considered a match\n",
    "    match_frac = 3/4\n",
    "    #Define empty lists to store the GLEAM data\n",
    "    ra_gleam = []\n",
    "    dec_gleam = []\n",
    "    imag_gleam = []\n",
    "    #Print an indicarion that the GLEAM file is being opened \n",
    "    print('Opening GLEAM File', end = \"\\r\")\n",
    "    #Create the path to the GLEAM file\n",
    "    gpath = glob.glob(GLEAM_path)\n",
    "    #Create an empty dictionary to store the GLEAM data\n",
    "    GLEAM_data = {}\n",
    "    #Open the data from the GLEAM file\n",
    "    GLEAM_data['data'] =  [scipy.io.readsav(gpath[i], python_dict=True)\n",
    "            for i in range(len(gpath))]\n",
    "    #Find the length of the GLEAM file\n",
    "    size = GLEAM_data['data'][0]['catalog'].shape[0]\\\n",
    "    #Create empty lists to store the extended objects in the GLEAM file\n",
    "    ra_gleam_ext = []\n",
    "    dec_gleam_ext = []\n",
    "    imag_gleam_ext = []\n",
    "    #Create a list to store the names associated with the GLEAM data\n",
    "    gnams = []\n",
    "    #Check if the data type is correct for interpretation\n",
    "    if type(GLEAM_data['data'][0]['catalog'][0][0]) == bytes:\n",
    "        #Loop over the sources in GLEAM\n",
    "        for z in range(size):\n",
    "            #Pull out the identifier for the source\n",
    "            s = GLEAM_data['data'][0]['catalog'][z]['ID']\n",
    "            #Get the important part of the identifier and make it a string\n",
    "            nam = str(s)[2:-1]\n",
    "            #Append the name to the GLEAM identifiers\n",
    "            gnams.append(nam)\n",
    "    #Load the GLEAM data\n",
    "    for i in range(size):\n",
    "        #Check if there are no extended components\n",
    "        if GLEAM_data['data'][0]['catalog'][i]['EXTEND'] is None:\n",
    "            #append RA to appropriate gleam list\n",
    "            ra_gleam.append(GLEAM_data['data'][0]['catalog'][i]['RA'])\n",
    "            #append DEC to appropriate gleam list\n",
    "            dec_gleam.append(GLEAM_data['data'][0]['catalog'][i]['DEC'])\n",
    "            #append Magnitude to appropriate gleam list\n",
    "            imag_gleam.append(GLEAM_data['data'][0]['catalog'][i]['FLUX'][flux_type])\n",
    "            #append a 0 to indicate there are no extened components to apropriate lists\n",
    "            ra_gleam_ext.append(0)\n",
    "            dec_gleam_ext.append(0)\n",
    "            imag_gleam_ext.append(0)\n",
    "        else:\n",
    "            #If there are extended sources\n",
    "            #append RA ,DEC, and magnitude to appropriate GLEAM list\n",
    "            ra_gleam.append(GLEAM_data['data'][0]['catalog'][i]['RA'])\n",
    "            dec_gleam.append(GLEAM_data['data'][0]['catalog'][i]['DEC'])\n",
    "            imag_gleam.append(GLEAM_data['data'][0]['catalog'][i]['FLUX'][flux_type])\n",
    "            #create an empty lsit to store the extended magnitude components\n",
    "            glm_mag_eo = []\n",
    "            #Loop through and pick out the extended points in I flux then append these to the list\n",
    "            for j in range(0, GLEAM_data['data'][0]['catalog'][i]['FLUX'].shape[0]):\n",
    "                glm_mag_eo.append(GLEAM_data['data'][0]['catalog'][i]['EXTEND']['FLUX'][j][flux_type])\n",
    "            imag_gleam_ext.append(glm_mag_eo)\n",
    "            #append the values for extended RA and DEC components\n",
    "            ra_gleam_ext.append(GLEAM_data['data'][0]['catalog'][i]['EXTEND']['RA'])\n",
    "            dec_gleam_ext.append(GLEAM_data['data'][0]['catalog'][i]['EXTEND']['DEC'])\n",
    "    #Create a Pandas Data Frame with the RA, DEC, and GLEAM Magnitudes\n",
    "    n=0\n",
    "    df = pd.DataFrame({'RA': ra_gleam,'Mag GLEAM': imag_gleam,  'DEC' : dec_gleam, 'RA EO GLEAM':ra_gleam_ext, \n",
    "                      'DEC EO GLEAM': dec_gleam_ext, 'Mag EO GLEAM': imag_gleam_ext})\n",
    "    #Look at each path in the directory\n",
    "    fl = 0\n",
    "    #Check if the optional keyword to grab only a few files has been entered, if so apropriately \n",
    "    #index the data\n",
    "    iter_paths = paths[0:index_lim] if index_lim else paths\n",
    "    for path in iter_paths:\n",
    "        fl +=1\n",
    "        os.system('clear')\n",
    "        print(f\"Generating table for file {fl} of {len(iter_paths)}\", end = \"\\r\")\n",
    "        #Collect the data for each path\n",
    "        n = n + 1\n",
    "        #Identify path to this data file\n",
    "        datpath = glob.glob(path)\n",
    "        #Create a dictionary to store the data\n",
    "        data = {}\n",
    "        #Open the data\n",
    "        data['data'] = [scipy.io.readsav(datpath[i], python_dict=True)\n",
    "            for i in range(len(datpath))]\n",
    "        #Create some empty dictionaries to store the data\n",
    "        eo = []\n",
    "        eo_ra = []\n",
    "        eo_dec = []\n",
    "        ps_RA = []\n",
    "        ps_DEC = []\n",
    "        i_mag = []\n",
    "        EO_imag = []\n",
    "        ps_ston = []\n",
    "        eo_ston = []\n",
    "        #Try using the 'catalog' keyword to open the data\n",
    "        try:\n",
    "            d_s = data['data'][0]['catalog']\n",
    "        #If the 'catalog' keyword does not exist, assume that the keyword is 'source_array'\n",
    "        except:\n",
    "            d_s = data['data'][0]['source_array']\n",
    "        if observation_freqs ==True:\n",
    "            obsv_freqs.append(round(np.mean(d_s['FREQ'])))\n",
    "        #Loop over the sources in the data file\n",
    "        for d in d_s:\n",
    "            #Check if the source has extended, or multiple, components\n",
    "            #If the source is not extended, store only the high-level\n",
    "            #source information and disregard extended keywords, adding \n",
    "            #these high-level values to the lists containing extended information\n",
    "            if d['EXTEND'] is None:\n",
    "                ps_RA.append(d['RA'])\n",
    "                ps_DEC.append(d['DEC'])\n",
    "                #Flux in I is stored and other polarized fluxes are ignored\n",
    "                EO_imag.append(d['FLUX'][flux_type])\n",
    "                eo_ra.append(d['RA'])\n",
    "                eo_dec.append(d['DEC'])\n",
    "                #Flux in I is stored and other polarized fluxes are ignored\n",
    "                i_mag.append(d['FLUX'][flux_type])\n",
    "                ps_ston.append(d['STON'])\n",
    "                eo_ston.append(d['STON'])\n",
    "            #If the source does have extended components, add the high level point-\n",
    "            #like values to high level lists and add extended components to\n",
    "            #the extended lists\n",
    "            else:\n",
    "                ps_RA.append(d['RA'])\n",
    "                ps_DEC.append(d['DEC'])\n",
    "                #Create empty lists to store information about extended flux and signal to noise\n",
    "                EOmags = []\n",
    "                eoston = []\n",
    "                #Loop over the extended flux to pull out only the relavent flux in I\n",
    "                for i in range(0, d['EXTEND']['FLUX'].shape[0]):\n",
    "                    #Flux in I is stored and other polarized fluxes are ignored\n",
    "                    EOmags.append(d['EXTEND']['FLUX'][i][flux_type])\n",
    "                    eoston.append(d['EXTEND']['STON'][i])\n",
    "                #Append these lists of \n",
    "                eo_ston.append(np.array(eoston))\n",
    "                EO_imag.append(np.array(EOmags))\n",
    "                eo_ra.append(d['EXTEND']['RA'])\n",
    "                eo_dec.append(d['EXTEND']['DEC'])\n",
    "                #Flux in I is stored and other polarized fluxes are ignored\n",
    "                i_mag.append(d['FLUX'][flux_type])\n",
    "                ps_ston.append(d['STON'])   \n",
    "        #Match this path with the GLEAM catalog\n",
    "        #idx: an array of indices corresponding to matches\n",
    "        #d2d: the two dimensional distances between these matches\n",
    "        #d3d: three dimensional distances between matches. This array is blank becasue we do \n",
    "        #not have 3 dimensional data, but the match_to_catalog_sky function requires it anyway\n",
    "        catalog = SkyCoord(ra=ps_RA*u.deg, dec=ps_DEC*u.deg)  \n",
    "        c = SkyCoord(ra=ra_gleam*u.deg, dec=dec_gleam*u.deg)  \n",
    "        idx, d2d, d3d = c.match_to_catalog_sky(catalog)   \n",
    "        \n",
    "        if missed_source_table == True:\n",
    "            check = np.arange(len(ps_RA))\n",
    "\n",
    "            missed_idx = []\n",
    "            for intg in np.arange(len(ps_RA)):\n",
    "                if intg not in check[idx]:\n",
    "                    missed_idx.append(intg)\n",
    "            missing_dat[f'observation{fl}'] = {}\n",
    "            missing_dat[f'observation{fl}']['RA'] = np.array(ps_RA)[missed_idx]\n",
    "            missing_dat[f'observation{fl}']['DEC'] = np.array(ps_DEC)[missed_idx]\n",
    "            missing_dat[f'observation{fl}']['MAG'] = np.array(i_mag)[missed_idx]\n",
    "            missing_dat[f'observation{fl}']['RAextended'] = np.array(pd.Series(eo_ra))[missed_idx]\n",
    "            missing_dat[f'observation{fl}']['DECextended'] = np.array(pd.Series(eo_dec))[missed_idx]\n",
    "            missing_dat[f'observation{fl}']['MAGextended'] = np.array(pd.Series(EO_imag))[missed_idx]\n",
    "            missing_dat[f'observation{fl}']['STON'] = np.array(ps_ston)[missed_idx]\n",
    "            missing_dat[f'observation{fl}']['STONextended'] = np.array(pd.Series(eo_ston))[missed_idx]\n",
    "            \n",
    "        #Only return matches within one degree \n",
    "        #Create an empty list to store the matched data\n",
    "        mags = []\n",
    "        #Sort the flux array with the the idx index array so that the magnitudes are ordered by match\n",
    "        imags = np.array(i_mag)[idx]\n",
    "        # create number indicies to loop over\n",
    "        nums = np.arange(0, len(idx))\n",
    "        #Loop through each index\n",
    "        for num in nums:\n",
    "            #Check if the matching distance is whithin the specefied distance\n",
    "            #Check if the flux is withing a specified fraction of the GLEAM magnitude \n",
    "            #to specify a match\n",
    "            if (d2d[num] < (match_dist*u.deg)) and ((imags[num] > imag_gleam[num] + (match_frac)*imag_gleam[num]) or (imags[num] > (match_frac)*imag_gleam[num])):\n",
    "                mags.append(imags[num])\n",
    "                #If there is no match, add a 0 in place of the flux vlaue\n",
    "            else: \n",
    "                mags.append(0)\n",
    "        #Add a new column to the data frame with the information from these observations \n",
    "        #Create a pandas data series of the flux array and the 2-dimensional distance arrays\n",
    "        #which have already been ordered correctly\n",
    "        s_mag = pd.Series(mags)\n",
    "        s_dist = pd.Series(d2d)\n",
    "        observation_df = pd.DataFrame({'Mag {}'.format(n): s_mag,'Distance {}'.format(n):s_dist, 'RA {}'.format(n):np.array(ps_RA)[idx],\n",
    "                          'DEC {}'.format(n):np.array(ps_DEC)[idx], 'EO RA {}'.format(n):np.array(pd.Series(eo_ra))[idx],\n",
    "                          'EO DEC {}'.format(n):np.array(pd.Series(eo_dec))[idx], 'EO Mag {}'.format(n):np.array(pd.Series(EO_imag))[idx],\n",
    "                          'STON {}'.format(n):np.array(ps_ston)[idx], 'EO STON {}'.format(n):np.array(pd.Series(eo_ston))[idx]})\n",
    "        \n",
    "        #Add a new column to the data frame for each list and fill it with the relavent data\n",
    "        \n",
    "       # df['Mag {}'.format(n)] = s_mag\n",
    "        #df['Distance {}'.format(n)] = s_dist\n",
    "        \n",
    "        #Order the remaining information corectly by indexing them with the idx arrays\n",
    "        #create a pandas dara series from these arrays to ensure that they are corectly formatted\n",
    "        \n",
    "        #df['RA {}'.format(n)] = np.array(ps_RA)[idx]\n",
    "        #df['DEC {}'.format(n)] = np.array(ps_DEC)[idx]\n",
    "        #df['EO RA {}'.format(n)] = np.array(pd.Series(eo_ra))[idx]\n",
    "        #df['EO DEC {}'.format(n)] = np.array(pd.Series(eo_dec))[idx]\n",
    "        #df['EO Mag {}'.format(n)] = np.array(pd.Series(EO_imag))[idx]\n",
    "        #df['STON {}'.format(n)] = np.array(ps_ston)[idx]\n",
    "        #df['EO STON {}'.format(n)] = np.array(pd.Series(eo_ston))[idx]\n",
    "        df = pd.concat([df, observation_df], axis = 1)\n",
    "    #change the index of the data frames to reflect the names of the objects specified by GLEAM\n",
    "    df.index = gnams\n",
    "    extras = {'Missed Sources Dictionary': missing_dat, 'Observation Frequencies':observ_freqs}\n",
    "    return df, extras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7136380",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_missing(missing_dict):\n",
    "    plt.rcParams['figure.figsize'] = (10, 10)\n",
    "    plt.rc('axes', labelsize=14)\n",
    "    plt.rc('axes', labelweight='bold')\n",
    "    plt.rc('axes', titlesize=16)\n",
    "    plt.rc('axes', titleweight='bold')\n",
    "    plt.rc('font', family='sans-serif')\n",
    "    for m in range(len(missing_dict.keys())):\n",
    "        m = m+1\n",
    "        plt.figure()\n",
    "        for source in range(len(missing_dict[f'observation{m}']['MAGextended'])):\n",
    "            if len(missing_dict[f'observation{m}']['MAGextended'][source]) >1:\n",
    "                ra = missing_dict[f'observation{m}']['RAextended'][source]\n",
    "                dec = missing_dict[f'observation{m}']['DECextended'][source]\n",
    "\n",
    "                ras = []\n",
    "                for r in ra:\n",
    "                    if r > 180:\n",
    "                        ras.append(r-360)\n",
    "                    else:\n",
    "                        ras.append(r)\n",
    "                plt.scatter(ras, dec, c= 'r', label = 'Unmatched', s = 50)\n",
    "\n",
    "\n",
    "            else:\n",
    "                ra = missing_dict[f'observation{m}']['RAextended'][source]\n",
    "                dec = missing_dict[f'observation{m}']['DECextended'][source]\n",
    "                if ra > 180:\n",
    "                    ra = ra-360\n",
    "                plt.scatter(ra, dec, c = 'r', s = 10, zorder = 10)\n",
    "                \n",
    "            #ras = []\n",
    "            #decs = []\n",
    "            #mags = df.loc[index][6::9]\n",
    "            #for i in range(0, len(mags)):\n",
    "               # if mags[i] !=0:\n",
    "                    #ras.append(df[f'RA {m}'][i])\n",
    "                   #decs.append(df[f'DEC {m}'] [i])\n",
    "                \n",
    "            #plt.scatter(df[f'RA {1}'], df[f'DEC {1}'], label = f'Observation {1}', alpha = 0.25, color = 'k', zorder = 0)\n",
    "            data_r = df[f'RA {m}']\n",
    "            data_dec = df[f'DEC {m}']\n",
    "            data_ra = []\n",
    "            for r in data_r:\n",
    "                if r > 180:\n",
    "                     data_ra.append(r-360)\n",
    "                else:\n",
    "                    data_ra.append(r)\n",
    "            plt.scatter(data_ra, data_dec, c = 'k', label = 'Matched to GLEAM Object', s = 10, zorder = 1)\n",
    "            plt.xlabel('RA (deg)')\n",
    "            plt.ylabel('DEC (deg)')\n",
    "            plt.title(f'Observation {m} Known GLEAM sources and Unmatched Objects')\n",
    "            plt.legend()\n",
    "            plt.gca().invert_xaxis();\n",
    "        \n",
    "        plt.figure()\n",
    "        plt.rcParams['figure.figsize'] = (15, 5)\n",
    "        plt.hist(missing_dict[f'observation{m}']['MAG'], bins = 100)\n",
    "        plt.xscale('log')\n",
    "        plt.title('Unmatched sources for Observation 1, Log scale')\n",
    "        plt.xlabel('Source Flux [Jy]');\n",
    "        \n",
    "        plt.figure()\n",
    "        plt.rcParams['figure.figsize'] = (15, 5)\n",
    "        plt.hist(missing_dict['observation1']['MAG'], bins = 80)\n",
    "        plt.title('Unmatched sources for Observation 1, linear scale')\n",
    "        plt.xlabel('Source Flux [Jy]');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4c4db42",
   "metadata": {},
   "source": [
    "## Generating Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72d17b4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gaussian(sigma, x_0, y_0, x, y, power):\n",
    "    \"\"\"\n",
    "    Returns the value at a given x and y position for a gaussian surface centered at x_0, y_0\n",
    "    \n",
    "    Parameters\n",
    "    ---\n",
    "    \n",
    "    sigma: float\n",
    "           standard deviation of the gaussain beam\n",
    "    \n",
    "    x_0: float\n",
    "         x value of peak \n",
    "    \n",
    "    y_0: float \n",
    "        y value of peak\n",
    "    \n",
    "    x: float \n",
    "      the x value of the coordinate to be calculated\n",
    "    \n",
    "    y: flaot\n",
    "       the y vlaue of the coordinate to be calculated\n",
    "    \n",
    "    power: float\n",
    "           amplitude of gaussian at peak\n",
    "    \n",
    "    Returns\n",
    "    ---\n",
    "    gauss_val: float\n",
    "                the value of the gaussian beam at the specified x, y position\n",
    "    \"\"\"\n",
    "    #xs is the squared difference between x and x_0\n",
    "    xs = (x-x_0)**2\n",
    "    #ys is the squared difference between y and y_0\n",
    "    ys = (y-y_0)**2\n",
    "    #gauss_val is the vlaue of a gaussin beam at the specified point\n",
    "    gauss_val = power * (e** ((-(xs + ys))/(2*(sigma**2))))\n",
    "    return gauss_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f607434",
   "metadata": {},
   "outputs": [],
   "source": [
    "def shift_arrays(table, index, save_plots = True):\n",
    "    \"\"\"\n",
    "    Shifts all arrays for a given observation towards an unweighted mean center. This is a good way to\n",
    "    quickly check what a source looks like without running the smoothing which is more time consuming\n",
    "    \n",
    "    Parameters\n",
    "    ---\n",
    "    \n",
    "    table: DataFrame\n",
    "           a table generated by the match_to_gleam function\n",
    "    \n",
    "    index: string or integer\n",
    "            the index of the target source and is an integer index or a string \n",
    "            \n",
    "    save_plots: bool, optional\n",
    "                Saves plots to current working directory\n",
    "    \n",
    "    Returns\n",
    "    ---\n",
    "    Displays and saves plot of source to the current working directory\n",
    "    \n",
    "    \"\"\"\n",
    "    #Settilng matplotlib to display a clean plot\n",
    "    plt.rcParams['figure.figsize'] = (10, 10)\n",
    "    plt.rc('axes', labelsize=14)\n",
    "    plt.rc('axes', labelweight='bold')\n",
    "    plt.rc('axes', titlesize=16)\n",
    "    plt.rc('axes', titleweight='bold')\n",
    "    plt.rc('font', family='sans-serif')\n",
    "    #Pull relavent data from the pandas DataFrame\n",
    "    mags = table.loc[index][6::9]\n",
    "    mags_eo = table.loc[index][12::9]\n",
    "    ra_center = table.loc[index][8::9]\n",
    "    dec_center = table.loc[index][9::9]\n",
    "    ext_ras = table.loc[index][10::9]\n",
    "    ext_decs = table.loc[index][11::9]\n",
    "    #Create empty lists to store shifted data\n",
    "    ras = []\n",
    "    decs = []\n",
    "    mag_change = []\n",
    "    adj_obs = []\n",
    "    imnum = []\n",
    "    true_eo_mags = []\n",
    "    #Create a number to track iterations\n",
    "    num = 0\n",
    "    #Loop through the individual observations of the source\n",
    "    for i in range(0, len(mags)):\n",
    "        #Add to the iteration tracker\n",
    "        num = num+1\n",
    "        #Check if the mags array is 0. If it is, this 0, then it was identified\n",
    "        #as not a match and we pass over the data\n",
    "        if mags[i] !=0:\n",
    "            #Append the values to the proper lists if recognized as a match\n",
    "            mag_change.append(mags[i])\n",
    "            ras.append(ra_center[i])\n",
    "            decs.append(dec_center[i])\n",
    "            imnum.append(num)\n",
    "    #Find the center for the x and y values by finding the mean, unweighted\n",
    "    x_center = np.mean(ras)\n",
    "    y_center = np.mean(decs)\n",
    "    #Loop through the observations again\n",
    "    for i in range(0, len(mags)):\n",
    "        #Check agian if the observation is a match\n",
    "        if mags[i] !=0:\n",
    "            #Grab the relavent RA and DEC values from the lists\n",
    "            rs = ext_ras[i]\n",
    "            ds = ext_decs[i]\n",
    "            #Calculate the distance the RA and DEC values must be shifted\n",
    "            ra_roll = x_center - ra_center[i]\n",
    "            dec_roll = y_center - dec_center[i]\n",
    "            #Create a new array containing the shifted RA and DEC vlaues\n",
    "            new_array = rs+ra_roll, ds+dec_roll\n",
    "            #Add the adjusted array to the list of new observations\n",
    "            adj_obs.append(new_array)\n",
    "            #Add the flux aray to the list of flux arrays\n",
    "            true_eo_mags.append(mags_eo[i])\n",
    "    #Loop over the identified and shifted observations\n",
    "    for r in range(len(adj_obs)):\n",
    "        #plot a scatter plot of the newly shifted observations\n",
    "        plt.scatter(adj_obs[r][0], adj_obs[r][1],# s = true_eo_mags[r]*5, \n",
    "            label = 'Observation {}, {} Jy'.format(imnum[r],mag_change[r]))\n",
    "    #Create X and Y labels, create title\n",
    "    plt.xlabel('RA (Degrees)')\n",
    "    plt.ylabel('DEC (Degrees)')\n",
    "    plt.title('Source {}, Center Calculation'.format(index))\n",
    "    #Identify the RA and DEC values originally identified in GLEAM\n",
    "    g_ra = table.loc[index]['RA']\n",
    "    g_dec = table.loc[index]['DEC']\n",
    "    #Use these to adjust the X and Y limits of the plot\n",
    "    plt.xlim(g_ra-.25, g_ra+.25)\n",
    "    plt.ylim(g_dec-.25, g_dec+.25)\n",
    "    #Add a grid to the plot\n",
    "    plt.grid()\n",
    "    #Invert the x axis\n",
    "    plt.gca().invert_xaxis()\n",
    "    #Add a legend\n",
    "    plt.legend()\n",
    "    #Save the figure to the current directory\n",
    "    plt.savefig(f'source{index}scatterplots.png');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9597ad0a",
   "metadata": {},
   "outputs": [],
   "source": [
    " def create_points(table, index):\n",
    "    \"\"\"\n",
    "    Returns point arrays from available observations for the identified source that give relavent\n",
    "    information about the source. This is intended to be given to the clust_points function.\n",
    "    \n",
    "    Parameters\n",
    "    ---\n",
    "    \n",
    "    table: DataFrame\n",
    "           a pandas data frame generated by the match_to_gleam function\n",
    "    \n",
    "    index: integer or string \n",
    "           an integer index or string identifier for the target source\n",
    "    \n",
    "    Returns\n",
    "    ---\n",
    "    clust_ras: list\n",
    "               list of lists. Each list is a series of Right Assention values in decimal degrees\n",
    "               of the extended points for this source. Each list represents a seperate observation\n",
    "    \n",
    "    clust_decs: list\n",
    "               list of lists. Each list is a series of Declination values in decimal degrees\n",
    "               of the extended points for this source. Each list represents a seperate observation\n",
    "    \n",
    "    clust_mags: list\n",
    "               list of lists. Each list is a series of flux values in  Jansky\n",
    "               of the extended points for this source. Each list represents a seperate observation\n",
    "               \n",
    "    pointmags: list\n",
    "               list of point-like flux of the source in Jansky \n",
    "    \n",
    "    x_axis: numpy array\n",
    "            Array representing the x-axis for the modeling area\n",
    "    \n",
    "    y_axis  numpy array\n",
    "            Array representing the y-axis for the modeling area\n",
    "    \"\"\"\n",
    "    #Set a vavriable to count the number of observations to zero\n",
    "    obscount = 0\n",
    "    #Create empty lists to store data\n",
    "    immags = []\n",
    "    imras = []\n",
    "    imdecs = []\n",
    "    upras = []\n",
    "    updecs = []\n",
    "    upmags = []\n",
    "    ra_obs = []\n",
    "    dec_obs = []\n",
    "    mag_obs = []\n",
    "    obs_data = []\n",
    "    ra_obs_smth = []\n",
    "    dec_obs_smth = []\n",
    "    mag_obs_smth = []\n",
    "    mag_change = []\n",
    "    image_ra = []\n",
    "    image_dec = []\n",
    "    image_mag = []\n",
    "    image_ston = []\n",
    "    observation_number = []\n",
    "    ps_mag = []\n",
    "    upper_ra = []\n",
    "    upper_dec = []\n",
    "    h_angs = []\n",
    "    otimes = []\n",
    "    true_o_num = []\n",
    "    adj_obs = []\n",
    "    raz = []\n",
    "    decz = []\n",
    "    true_eo_mags = []\n",
    "    #Identify the RA and DEC values of the original GLEAM objects\n",
    "    gleam_ra = table.loc[index]['RA']\n",
    "    gleam_dec = table.loc[index]['DEC']\n",
    "    #mags are point source magnitudeds\n",
    "    mags = table.loc[index][6::9]\n",
    "    #mags_eo are the extended magnitude components\n",
    "    mags_eo = table.loc[index][12::9]\n",
    "    #ra for the extended components\n",
    "    ras = np.array(table.loc[index][10::9])\n",
    "    #dec for extended components\n",
    "    decs = np.array(table.loc[index][11::9])\n",
    "    #Singal to noise as a  point source value\n",
    "    STON_ps = table.loc[index][13::9]\n",
    "    #Grab the extended RA and DEC for the source\n",
    "    high_ra = table.loc[index][8::9]\n",
    "    high_dec = table.loc[index][9::9]\n",
    "    #Create a variable to flag large sources\n",
    "    large_source_flag = 0\n",
    "    #Loop over the observations in the table\n",
    "    for i in range(0, len(mags)):\n",
    "        #Check if the observation identified a match\n",
    "        if mags[i] !=0:\n",
    "            #Add the point-like magnitude to a list\n",
    "            mag_change.append(mags[i])\n",
    "            #Increase the conter for the number of observations\n",
    "            obscount+=1\n",
    "            #Add the extended RA and DEC values to the list\n",
    "            raz.append(high_ra[i])\n",
    "            decz.append(high_dec[i])\n",
    "            #Check if the source spills out of the 0.5 by 0.5 degree modeling area, flag if so\n",
    "            if abs(np.max(ras[i]) - np.min(ras[i]))> 0.5:\n",
    "                large_source_flag +=1\n",
    "            if abs(np.max(decs[i]) - np.min(decs[i]))> 0.5:\n",
    "                large_source_flag +=1\n",
    "    #Create the x and y axes, depending on if the source requires a larger modeling area\n",
    "    if large_source_flag == 0:\n",
    "        x_stretch = .25\n",
    "        y_stretch = .25\n",
    "    else:\n",
    "        x_stretch = .35\n",
    "        y_stretch = .35\n",
    "    #Create min and max x and y axis values\n",
    "    xmin = gleam_ra - x_stretch\n",
    "    xmax = gleam_ra + x_stretch\n",
    "    ymin = gleam_dec - y_stretch\n",
    "    ymax = gleam_dec + y_stretch\n",
    "    #Create x and y axes \n",
    "    x_axis = np.linspace(xmin - 0.001, xmax + 0.001, num=100, endpoint=True)\n",
    "    y_axis = np.linspace(ymin - 0.001, ymax + 0.001, num=100, endpoint=True)\n",
    "    #Find the source center in degrees from the identified observations\n",
    "    x_center = np.mean(raz)\n",
    "    y_center = np.mean(decz)\n",
    "    #Loop over the seperate obersvations\n",
    "    for i in range(0, len(mags)):\n",
    "        #Check if the observation is a match\n",
    "        if mags[i] !=0:\n",
    "            #Append the RA and DEC values to the apropriate list\n",
    "            rs = ras[i]\n",
    "            ds = decs[i]\n",
    "            #Move the arrays towards the calculated center\n",
    "            ra_roll = x_center - high_ra[i]\n",
    "            dec_roll = y_center - high_dec[i]\n",
    "            #Create an array with the adjusted observations\n",
    "            new_array = rs+ra_roll, ds+dec_roll\n",
    "            #Add array with RA and DEc adjustments to master list\n",
    "            adj_obs.append(new_array)\n",
    "            #Add the flux arrays to a list\n",
    "            true_eo_mags.append(mags_eo[i])\n",
    "    #Loop over the observations\n",
    "    for n in range(0, len(mags)):\n",
    "        #Check if the observation is identified as a match\n",
    "        if mags[n] !=0:\n",
    "            #Store the data for each match. \n",
    "            #the image arrays are arrays made of lists with one list for each match \n",
    "            image_ra.append(ras[n])\n",
    "            image_dec.append(decs[n])\n",
    "            image_mag.append(mags_eo[n])\n",
    "            observation_number.append(n)\n",
    "            image_ston.append(STON_ps[n])\n",
    "            ps_mag.append(mags[n])\n",
    "            upper_ra.append(high_ra[n])\n",
    "            upper_dec.append(high_dec[n])\n",
    "    #Calculate the mean image center\n",
    "    center_ra = np.mean(upper_ra)\n",
    "    center_dec = np.mean(upper_dec)\n",
    "    #Create empty lists to store the data\n",
    "    clust_ras = []\n",
    "    clust_decs = []\n",
    "    clust_mags = []\n",
    "    pointmags  = []\n",
    "    #Loop through the identified data and add relavent data to the apropriate list\n",
    "    for m in range(0, len(adj_obs)):\n",
    "        clust_ras.append(adj_obs[m][0])\n",
    "        clust_decs.append(adj_obs[m][1])\n",
    "        clust_mags.append(image_mag[m])\n",
    "        pointmags.append(ps_mag[m])\n",
    "    return clust_ras, clust_decs, clust_mags, pointmags, x_axis, y_axis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4d5fbf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clust_points(clust_ras, clust_decs, clust_mags, pointmags, radius, percent_total):\n",
    "    \"\"\"\n",
    "    Creates clustered source models given specified input data. This function is meant to be \n",
    "    called by the create_plots function.\n",
    "    \n",
    "    This function clusters points to identify portions of an object that have potential to be point-like. \n",
    "    Each observation of a source is considered individualy. The function will loop through all points within \n",
    "    an obesrvation and place a circle around these points of the speciified radius. Other points in the \n",
    "    observation that fit within this radius are considered together. If this cluster of points has a \n",
    "    total flux in Jy greater than some percent of the total source flux, specified by percent_total, all \n",
    "    points within this radius are clustered together into a signle point and removed from the loop.\n",
    "    \n",
    "    Parameters\n",
    "    ---\n",
    "    \n",
    "    clust_ras: list of lists. Each list is a series of Right Assention values in decimal degrees\n",
    "               of the extended points for this source. Each list represents a seperate observation\n",
    "    \n",
    "    clust_decs:list of lists. Each list is a series of Declination values in decimal degrees\n",
    "               of the extended points for this source. Each list represents a seperate observation\n",
    "    \n",
    "    clust_mags:list of lists. Each list is a series of flux values in  Jansky\n",
    "               of the extended points for this source. Each list represents a seperate observation\n",
    "               \n",
    "    pointmags: list of point-like flux of the source in Jansky \n",
    "    \n",
    "    x_axis: Array representing the x-axis for the modeling area\n",
    "    \n",
    "    y_axis  Array representing the y-axis for the modeling area\n",
    "    \n",
    "    \n",
    "    radius: the radius of the circle that will determine if objects are point like in decimal degrees\n",
    "    \n",
    "    percent_total: the percent cutoff, in decimal percentage, of the total object brightness \n",
    "    that will determine if an object is point-like\n",
    "    \n",
    "    Returns\n",
    "    ---\n",
    "    pointlike_dict: a dictionary containing information about the clustered points. The keys have the \n",
    "                    following format\n",
    "                    \n",
    "                    obsx:The highest-level keys have format obsx where x is an integer indicating the \n",
    "                         observation numer to which the points within that key belong\n",
    "                    \n",
    "                    component_number_x: These keys are contained within the observation keywords. x indicates \n",
    "                                        the component number associated with this point within the observation\n",
    "                                        and is an integer\n",
    "                                        \n",
    "                    The remaining keywords are a dictionary stored wihtin the component_number_x keys\n",
    "                    \n",
    "                    sum_match: the total flux in Jy that went into the particular component\n",
    "                    \n",
    "                    component_ras: the RA positions in decimal degrees of the points that atributed to\n",
    "                                   the component \n",
    "                                   \n",
    "                    component_decs: the DEC positions in decimal degrees of the points that atributed to\n",
    "                                   the component \n",
    "                                   \n",
    "                    component_mags: the Mag positions in Jy of the points that atributed to\n",
    "                                   the component \n",
    "                    \n",
    "                    component_ind: the integer indices of the points that atributed to\n",
    "                                   the component \n",
    "                                   \n",
    "                    weight_ra: the weighted position in decimal degrees of the RA for the component\n",
    "                    \n",
    "                    weight_dec: the weighted position in decimal degrees of the DEC for the component\n",
    "                    \n",
    "    untouched_ra: the original input RA values in decimal degrees\n",
    "    \n",
    "    untouched_dec: the original input DEC values in decimal degrees\n",
    "    \n",
    "    untouched_mag: the original input flux values in Jy\n",
    "    \"\"\"\n",
    "    #Creating several empty lists\n",
    "    onums = []\n",
    "    untouched_ra = []\n",
    "    untouched_dec = []\n",
    "    untouched_mag = []\n",
    "    colorlims = []\n",
    "    obsdatasmth = []\n",
    "    untouched_ra = []\n",
    "    untouched_dec = []\n",
    "    untouched_mag = []\n",
    "    #Try\n",
    "    try:\n",
    "        #Create an empty dictionary\n",
    "        pointlike_dict = {}\n",
    "        #Loop over the number of total observations of the source identified\n",
    "        #by the previously called functions\n",
    "        for m in range(0, len(clust_ras)):\n",
    "            #Add an integer to indicate the number of observations\n",
    "            onums.append(m)\n",
    "            #Identify the RA, DEC, and flux values from the input data\n",
    "            ra = clust_ras[m]\n",
    "            dec = clust_decs[m]\n",
    "            mag = clust_mags[m]\n",
    "            #Identify the high-level point-like flux for the source\n",
    "            point_mag = pointmags[m]\n",
    "            #Save these input arrays so that the function can output the original\n",
    "            #input point clusters \n",
    "            untouched_ra.append(ra)\n",
    "            untouched_dec.append(dec)\n",
    "            untouched_mag.append(mag)\n",
    "            #Identify the point-like flux value\n",
    "            upper_mag = pointmags[m]\n",
    "            #Create empty lists to store the data\n",
    "            match_ra = []\n",
    "            match_dec = []\n",
    "            match_mag = []\n",
    "            match_ind = []\n",
    "            smth_ra = []\n",
    "            smth_dec = []\n",
    "            smth_mag = []\n",
    "            m_ras = []\n",
    "            m_decs = []\n",
    "            m_mags = []\n",
    "            s_ras = []\n",
    "            s_decs = []\n",
    "            s_mags = []\n",
    "            idx = []\n",
    "            #Create two variables that can be adjusted to track iterations\n",
    "            n = 0\n",
    "            iterations = 0\n",
    "            #Add the first key to the dictionary, indicated by the observation number\n",
    "            pointlike_dict[f'obs{m}'] = {}\n",
    "            #Try here will catch all cases except those that are point-like observations\n",
    "            try:\n",
    "                #Loop through individual points in this observation\n",
    "                for r in range(0, len(ra)):\n",
    "                    #Add the flux value to a list for tracking the flux limits\n",
    "                    colorlims.append(mag[r][0])\n",
    "                    #Check if this point is in the list match_ra\n",
    "                    if ra[r] in match_ra:\n",
    "                        #If it is, do nothing.\n",
    "                        s = 0\n",
    "                    #If the point does not already exist in the match_ra list, we must attepmt to\n",
    "                    #cluster it\n",
    "                    else:\n",
    "                        #Increase n to track the number of iterations for clustering\n",
    "                        n +=1\n",
    "                        #Define the first coordinate to be considered\n",
    "                        #x and y are the positions in decimal degrees\n",
    "                        x = ra[r]\n",
    "                        y = dec[r]\n",
    "                        #power is the flux of the point\n",
    "                        power = mag[r][0]\n",
    "                        #xmatch, ymatch, and magmatch are empty lists to track matches\n",
    "                        xmatch = []\n",
    "                        ymatch = []\n",
    "                        magmatch = []\n",
    "                        #idxq tracks the indices of these points\n",
    "                        idxq = []\n",
    "                        #iterations is increased by one to track\n",
    "                        iterations = iterations + 1\n",
    "                        #We next loop through all other points in the observation\n",
    "                        for q in range (0, len(ra)):\n",
    "                            #Check if this point has already been pulled out by a previous iteration of the loop\n",
    "                            if q in match_ind:\n",
    "                                #If it has been identified, do nothing.\n",
    "                                s = 0\n",
    "                            #If it has not been identified, the point is considered\n",
    "                            else:\n",
    "                                #Calculate the distance between the x,y point defined above and this particular \n",
    "                                #point in the rest of the data\n",
    "                                dist = np.sqrt((((ra[q]) - x)**2) + (((dec[q]) - y)**2))\n",
    "                                #Check if the distance is less than the clustering radius\n",
    "                                if dist < radius:\n",
    "                                    #Add information about points that pass this check to the relavent lists\n",
    "                                    #idxq tracks the index of these matches\n",
    "                                    idxq.append(q)\n",
    "                                    #xmatch and ymatch track the RA and DEC values in decimal degrees\n",
    "                                    xmatch.append(ra[q])\n",
    "                                    ymatch.append(dec[q])\n",
    "                                    #magmatch tracks the flux value in Jy \n",
    "                                    magmatch.append(mag[q][0])\n",
    "                            #Check if the sum of the flux of all points identified in the previous loop \n",
    "                            #are greater than the percentage cutoff indicated at function call to indicate\n",
    "                            #that the cluster should be pulled out\n",
    "                            if (np.sum(magmatch) > (percent_total*(np.sum(upper_mag)))):\n",
    "                                #If these points will be pulled from the list, their indices as tracked by\n",
    "                                #the idxq array are added to the master index array idx\n",
    "                                for num in idxq:\n",
    "                                    idx.append(num)\n",
    "                                #The position coordinates, flux arrays, and indices are added to\n",
    "                                #lists specifically tracked by this loop iteration\n",
    "                                match_ra.extend(xmatch)\n",
    "                                match_dec.extend(ymatch)\n",
    "                                match_mag.extend(magmatch)\n",
    "                                match_ind.extend(idxq)\n",
    "                                #The mean position in RA is calculated, weighted by the point flux values\n",
    "                                mean_pos_ra = np.average(xmatch, weights = magmatch)\n",
    "                                #The mean position in DEC is calculated, weighted by the point flux values\n",
    "                                mean_pos_dec = np.average(ymatch, weights = magmatch)\n",
    "                                #A keyword within the key for this observation is created for this point.\n",
    "                                #The point dictionary has information about the total flux clustered in\n",
    "                                #this point as well as the position and flux of the components that went\n",
    "                                #into it and the indices of these points in the larger array. The mean\n",
    "                                #weighted position is also indicated.\n",
    "                                pointlike_dict[f'obs{m}'][f'component_number_{r}'] = {'sum_match': np.sum(magmatch), \n",
    "                                                                            'component_ras' : xmatch, \n",
    "                                                                            'component_decs' : ymatch,\n",
    "                                                                            'component_mags' : magmatch,\n",
    "                                                                            'component_ind':idxq, \n",
    "                                                                            'weight_ra':mean_pos_ra,\n",
    "                                                                            'weight_dec': mean_pos_dec}\n",
    "            #Except case here is for when the observation has a single point, and so nothing will be \n",
    "            #clustered but the single point is still added to the master dictionary for this source\n",
    "            except:\n",
    "                #This will be the only component in this observation as it is point-like.\n",
    "                #The information about the point is directly stored with 0s in the \n",
    "                #component_x keywords indicating that no points went into this cluster\n",
    "                r = 0\n",
    "                pr = ra\n",
    "                pd = dec\n",
    "                power = mag[0]\n",
    "                pointlike_dict[f'obs{m}'][f'component_number_{r}'] = {'sum_match': power, \n",
    "                                                            'component_ras' : [0],\n",
    "                                                            'component_decs' : [0],\n",
    "                                                            'component_mags' : [0],\n",
    "                                                            'component_ind':[0], 'weight_ra':pr,\n",
    "                                                            'weight_dec': pd}\n",
    "            #Try here catched all cases where the observation is not already point-like\n",
    "            try:\n",
    "                #create an empty list for indices to be stored\n",
    "                match_idx = []\n",
    "                #Loop through the components in this observation's dictionary\n",
    "                for key in pointlike_dict[f'obs{m}'].keys():\n",
    "                    #Extend the list with the indices of the components that went in to the point\n",
    "                    match_idx.extend(pointlike_dict[f'obs{m}'][key]['component_ind'])\n",
    "                #Create a master variable with all indices of all points that went in to point-like\n",
    "                #structure for the observation\n",
    "                n = list(set(match_idx))\n",
    "                #Print an indication of how many loop iterations ran, how many points from the original\n",
    "                #observation were extracted, and the total original points\n",
    "                print('Total Points', len(ra), 'Number Extracted', len(ra[n]), end = \"\\r\")\n",
    "                print( 'Total Iterations', np.sum(iterations), end = \"\\r\")\n",
    "            #Except catched the case where the observation was already a single point\n",
    "            except:\n",
    "                #Print an indication that the observation was not iterated over and is point-like\n",
    "                print('Point Like Observation', end = \"\\r\")\n",
    "    #Except catches            \n",
    "    except:\n",
    "        #Create a variable m that can be used to track total observations\n",
    "        m = 0\n",
    "        #Define an empty dictionary to store information about clustering\n",
    "        pointlike_dict = {}\n",
    "        #Add the current integer observation number to the number of observations\n",
    "        onums.append(m)\n",
    "        #Identify RA, DEC, and flux of the clusters of points within this observation\n",
    "        ra = clust_ras\n",
    "        dec = clust_decs\n",
    "        mag = clust_mags\n",
    "        #Identify the high-level point-like flux value in Jy\n",
    "        point_mag = pointmags\n",
    "        #Story the untouched RA, DEC, and Flux values\n",
    "        untouched_ra.append(ra)\n",
    "        untouched_dec.append(dec)\n",
    "        untouched_mag.append(mag)\n",
    "        #Store high-level point-lke flux\n",
    "        upper_mag = pointmags\n",
    "        #Define empty lists for storing the data\n",
    "        match_ra = []\n",
    "        match_dec = []\n",
    "        match_mag = []\n",
    "        match_ind = []\n",
    "        smth_ra = []\n",
    "        smth_dec = []\n",
    "        smth_mag = []\n",
    "        m_ras = []\n",
    "        m_decs = []\n",
    "        m_mags = []\n",
    "        s_ras = []\n",
    "        s_decs = []\n",
    "        s_mags = []\n",
    "        idx = []\n",
    "        #Create variables to track iterations\n",
    "        n = 0\n",
    "        iterations = 0\n",
    "        #Create a keywords in the dictionary for this observation\n",
    "        pointlike_dict[f'obs{m}'] = {}\n",
    "        #Try here will catch all cases except those that are point-like observations\n",
    "        try:\n",
    "            #Loop through individual points in this observation\n",
    "            for r in range(0, len(ra)):\n",
    "                #Check if this point is in the list match_ra\n",
    "                if ra[r] in match_ra:\n",
    "                    #If it is, do nothing\n",
    "                    s = 0\n",
    "                #If it has not been identified, the point is considered\n",
    "                else:\n",
    "                    #Increase n to track iterations for clusterig\n",
    "                    n +=1\n",
    "                    #Define the first coordinate to be considered\n",
    "                    #x and y are the positions in decimal degrees\n",
    "                    x = ra[r]\n",
    "                    y = dec[r]\n",
    "                    #power is the flux of the point\n",
    "                    power = mag[r]\n",
    "                    #xmatch, ymatch, and magmatch are empty lists to track matches\n",
    "                    xmatch = []\n",
    "                    ymatch = []\n",
    "                    magmatch = []\n",
    "                    #idxq tracks the indices of these points\n",
    "                    idxq = []\n",
    "                    #iterations is increased by one to track\n",
    "                    iterations = iterations + 1\n",
    "                    #We next loop through all other points in the observation\n",
    "                    for q in range (0, len(ra)):\n",
    "                        #Check if this point has already been pulled out by a previous iteration of the loop\n",
    "                        if q in match_ind:\n",
    "                            #If it has been identified, do nothing.\n",
    "                            s = 0\n",
    "                        else:\n",
    "                            #Calculate the distance between the x,y point defined above and this particular \n",
    "                            #point in the rest of the data\n",
    "                            dist = np.sqrt((((ra[q]) - x)**2) + (((dec[q]) - y)**2))\n",
    "                            #Check if the distance is less than the clustering radius\n",
    "                            if dist < radius:\n",
    "                                #Add information about points that pass this check to the relavent lists\n",
    "                                #idxq tracks the index of these matches\n",
    "                                idxq.append(q)\n",
    "                                #xmatch and ymatch track the RA and DEC values in decimal degrees\n",
    "                                xmatch.append(ra[q])\n",
    "                                ymatch.append(dec[q])\n",
    "                                magmatch.append(mag[q])\n",
    "                        #Check if the sum of the flux of all points identified in the previous loop \n",
    "                        #are greater than the percentage cutoff indicated at function call to indicate\n",
    "                        #that the cluster should be pulled out\n",
    "                        if (np.sum(magmatch) > (percent_total*(np.sum(upper_mag)))):\n",
    "                            #If these points will be pulled from the list, their indices as tracked by\n",
    "                            #the idxq array are added to the master index array idx\n",
    "                            for num in idxq:\n",
    "                                idx.append(num)\n",
    "                            #The position coordinates, flux arrays, and indices are added to\n",
    "                            #lists specifically tracked by this loop iteration\n",
    "                            match_ra.extend(xmatch)\n",
    "                            match_dec.extend(ymatch)\n",
    "                            match_mag.extend(magmatch)\n",
    "                            match_ind.extend(idxq)\n",
    "                            #The mean position in RA is calculated, weighted by the point flux values\n",
    "                            mean_pos_ra = np.average(xmatch, weights = magmatch)\n",
    "                            #The mean position in DEC is calculated, weighted by the point flux values\n",
    "                            mean_pos_dec = np.average(ymatch, weights = magmatch)\n",
    "                            #A keyword within the key for this observation is created for this point.\n",
    "                            #The point dictionary has information about the total flux clustered in\n",
    "                            #this point as well as the position and flux of the components that went\n",
    "                            #into it and the indices of these points in the larger array. The mean\n",
    "                            #weighted position is also indicated.\n",
    "                            pointlike_dict[f'obs{m}'][f'component_number_{r}'] = {'sum_match': np.sum(magmatch), \n",
    "                                                                        'component_ras' : xmatch, \n",
    "                                                                        'component_decs' : ymatch,\n",
    "                                                                        'component_mags' : magmatch,\n",
    "                                                                        'component_ind':idxq, \n",
    "                                                                        'weight_ra':mean_pos_ra,\n",
    "                                                                        'weight_dec': mean_pos_dec}\n",
    "        #Except case here is for when the observation has a single point, and so nothing will be \n",
    "        #clustered but the single point is still added to the master dictionary for this source\n",
    "        except:\n",
    "            #This will be the only component in this observation as it is point-like.\n",
    "            #The information about the point is directly stored with 0s in the \n",
    "            #component_x keywords indicating that no points went into this cluster\n",
    "            r = 0    \n",
    "            pr = ra\n",
    "            pd = dec\n",
    "            power = mag[0]\n",
    "            pointlike_dict[f'obs{m}'][f'component_number_{r}'] = {'sum_match': power, \n",
    "                                                        'component_ras' : [0],\n",
    "                                                        'component_decs' : [0],\n",
    "                                                        'component_mags' : [0],\n",
    "                                                        'component_ind':[0], 'weight_ra':pr,\n",
    "                                                        'weight_dec': pd}\n",
    "        #Try here catched all cases where the observation is not already point-like\n",
    "        try:    \n",
    "            #create an empty list for indices to be stored\n",
    "            match_idx = []\n",
    "            #Loop through the components in this observation's dictionary\n",
    "            for key in pointlike_dict[f'obs{m}'].keys():\n",
    "                #Extend the list with the indices of the components that went in to the point\n",
    "                match_idx.extend(pointlike_dict[f'obs{m}'][key]['component_ind'])\n",
    "            #Create a master variable with all indices of all points that went in to point-like\n",
    "            #structure for the observation\n",
    "            n = list(set(match_idx))\n",
    "            #Print an indication of how many loop iterations ran, how many points from the original\n",
    "            #observation were extracted, and the total original points\n",
    "            print('Total Points', len(ra), 'Number Extracted', len(ra[n]), end = \"\\r\")\n",
    "            print( 'Total Iterations', np.sum(iterations), end = \"\\r\")\n",
    "            #Except catched the case where the observation was already a single point\n",
    "        except:\n",
    "            #Print an indication that the observation was not iterated over and is point-like\n",
    "            print('Point Like Observation', end = \"\\r\")\n",
    "    return pointlike_dict, untouched_ra, untouched_dec, untouched_mag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1ceca9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_plots(table, index):\n",
    "    \"\"\"\n",
    "    Generates model with relavent plot for indicated source\n",
    "    \n",
    "    Parameters\n",
    "    ---\n",
    "    \n",
    "    table: pandas DataFrame generated by the match_to_GLEAM function\n",
    "    \n",
    "    index: integer or string index identifying the source a model is to be generated for \n",
    "    \n",
    "    Returns\n",
    "    ---\n",
    "    \n",
    "    \"\"\"\n",
    "    #Formatting Matplotlib\n",
    "    plt.rcParams['figure.figsize'] = (10, 10)\n",
    "    plt.rc('axes', labelsize=14)\n",
    "    plt.rc('axes', labelweight='bold')\n",
    "    plt.rc('axes', titlesize=16)\n",
    "    plt.rc('axes', titleweight='bold')\n",
    "    plt.rc('font', family='sans-serif')\n",
    "    #Seting the radius for the circle to determine what is pointlike\n",
    "    radius = 0.000277778*20\n",
    "    #Determining the percentage of total source brightness used to determine what is pointlike\n",
    "    percent_total = .15\n",
    "    #Creating the necessary point arrays and x and y axis dimmensions for this source\n",
    "    cx, cy, cz, cp, x_axis, y_axis = create_points(table, index)\n",
    "    #Getting the pointlike sources and untouched point arrays\n",
    "    pointlike_dict, untouched_ra, untouched_dec, untouched_mag = clust_points(cx, cy, cz, cp, radius, percent_total)\n",
    "    #Creating a check to determine what was pulled out\n",
    "    point_check = 0\n",
    "    #Check for source type and indicate if source falls outside of print statements implemented by\n",
    "    #previously called functions\n",
    "    for c in cx:\n",
    "        if type(c) != np.ndarray:\n",
    "            point_check +=1\n",
    "    if pointlike_dict == {}:\n",
    "        return print('Source Not Observed', end = \"\\r\")\n",
    "    elif point_check == len(cx):\n",
    "        return print('Source is Observed to be Pointlike', end = \"\\r\")\n",
    "    else:\n",
    "        #Defining some empty lists to store observation data\n",
    "        obs_data = []\n",
    "        upper_obs_data = []\n",
    "        #Create two variables that can be increased with the loops\n",
    "        n = -1\n",
    "        numobs = 0\n",
    "        # Going through each observation of the source\n",
    "        for dat in range(len(pointlike_dict)):\n",
    "            #Defining empty lists to store information\n",
    "            upper_ra_points = []\n",
    "            upper_dec_points = []\n",
    "            upper_mag_points = []\n",
    "            contribra = []\n",
    "            contribdec = []\n",
    "            contribmag = []\n",
    "            imgras = []\n",
    "            imgdecs = []\n",
    "            imgmags = []\n",
    "            n = n+1\n",
    "            match_idx = []\n",
    "            #Open each of the individual components of the observation that was pulled into the \n",
    "            #point-like dictionaries generated by the clust_points function\n",
    "            for key in pointlike_dict[f'obs{n}'].keys():\n",
    "                upper_ra_points.append(pointlike_dict[f'obs{n}'][key]['weight_ra'])\n",
    "                upper_dec_points.append(pointlike_dict[f'obs{n}'][key]['weight_dec'])\n",
    "                upper_mag_points.append(pointlike_dict[f'obs{n}'][key]['sum_match'])\n",
    "                contribra.extend(pointlike_dict[f'obs{n}'][key]['component_ras'])\n",
    "                contribdec.extend(pointlike_dict[f'obs{n}'][key]['component_decs'])\n",
    "                contribmag.extend(pointlike_dict[f'obs{n}'][key]['component_mags'])\n",
    "            #Try here catches cases where there are points recorded for the observation \n",
    "            try:\n",
    "                #loop through the untouched data\n",
    "                for coord in range(len(untouched_ra[dat])):\n",
    "                    #check if the untouched component contributed to the point sources and add to\n",
    "                    # the data arrays if they did not\n",
    "                    if untouched_ra[dat][coord] not in np.array(contribra):\n",
    "                        imgras.append(untouched_ra[dat][coord])\n",
    "                        imgdecs.append(untouched_dec[dat][coord])\n",
    "                        imgmags.append(untouched_mag[dat][coord])\n",
    "                #Create a dictionary containing the weighted clustered points\n",
    "                pointobs = {'ra': upper_ra_points, 'dec': upper_dec_points, 'mag': upper_mag_points}\n",
    "                #append the dictionary to a list of dictionaries\n",
    "                upper_obs_data.append(pointobs)\n",
    "                #increase the number of observations\n",
    "                numobs += 1\n",
    "                #Create an empty 100x100 matrix to store the data\n",
    "                data = np.matrix(np.zeros((100,100)))\n",
    "                #loop through the points that were ot pulled into clusters\n",
    "                for val in range (0, len(imgdecs)):\n",
    "                        #Identify the point positional values and flux\n",
    "                        smooth_x = imgras[val]\n",
    "                        smooth_y = imgdecs[val]\n",
    "                        smooth_m = imgmags[val]\n",
    "                        #Enumerate the x and y axes, allowing us to loop through all the points in the data \n",
    "                        #matrix\n",
    "                        for xind, xval in enumerate(x_axis):\n",
    "                            for yind, yval in enumerate(y_axis):\n",
    "                                #Set the value for the data matrix at this point to include the value of the \n",
    "                                #gaussian beam created by the point we are currently looping over \n",
    "                                data[xind, yind] += gaussian(1.5*(x_axis[1]-x_axis[0]), smooth_x, smooth_y, xval, yval, smooth_m)\n",
    "            #Except here catches the case when there are no components to smooth out and returns an empty \n",
    "            #data matrix\n",
    "            except:\n",
    "                data = np.matrix(np.zeros((100,100)))\n",
    "            #Append the data matrix, whether it be blank or containing smoothed gaussian surfaces,\n",
    "            #to a list containing the data arrays from the observations\n",
    "            obs_data.append(data)\n",
    "            #Create a matplotlib figure\n",
    "            fig, ax = plt.subplots(figsize = (10,10))\n",
    "            #Plot the data matrix generated by the most recent observation\n",
    "            implot = plt.imshow(np.flip(data.T, axis = 0), cmap = 'pink', interpolation = None, origin = 'upper', \n",
    "                               extent = [np.min(x_axis), np.max(x_axis), np.min(y_axis), np.max(y_axis)]) #, origin = 'lower', interpolation = None)#'gist_ncar')\n",
    "            #Add a colorbar\n",
    "            plt.colorbar()\n",
    "            #Create a scatter plot over this image containing all points that have been clustered out of\n",
    "            #the observation\n",
    "            plt.scatter(upper_ra_points, upper_dec_points, color = 'gold', s = upper_mag_points)\n",
    "            #Invert the RA values\n",
    "            plt.gca().invert_xaxis()\n",
    "            #Create a title for the observation plot indicating the observation number\n",
    "            plt.title(f'Observation {dat}, First Iteration')\n",
    "            #Add x and y labels\n",
    "            plt.xlabel('RA (Deg)')\n",
    "            plt.ylabel('DEC (Deg)')\n",
    "            #Create a unique file name containing the source index, observation number, clustering radius,\n",
    "            #and flux percentage cut-off. The image is displayed and saved to the current working directory\n",
    "            plt.savefig(f'source{index}observation{dat}radius{radius}bright{percent_total}scatter.png')\n",
    "            #Display figure\n",
    "            plt.show()\n",
    "            #Create a fresh figure\n",
    "            plt.figure()\n",
    "        #Grag the values from the data frame\n",
    "        mags = table.loc[index][6::9]#upper level magnitude values\n",
    "        ra_center = table.loc[index][8::9]#upper level ra values\n",
    "        dec_center = table.loc[index][9::9]#upper level dec values\n",
    "        ext_ras = table.loc[index][10::9] #lower level RA values\n",
    "        ext_decs = table.loc[index][11::9]#lower level DEC values\n",
    "        #Find the dimmensions of the square pixel\n",
    "        pixel_dim = x_axis[1]-x_axis[0]\n",
    "        #create some empty lists to store data\n",
    "        shifts = []\n",
    "        newdata = []\n",
    "        #Create a stacking variable and innitiate it as None\n",
    "        imstack = None \n",
    "        #Loop through the data matricies\n",
    "        for img in range(len(obs_data)):\n",
    "            #Grab the observation\n",
    "            shifted_obs = np.expand_dims(obs_data[img], axis = 0)\n",
    "            #Check if this is the first iteration\n",
    "            if imstack is None:\n",
    "                #If it is the first iteration, simply add this to the data stack\n",
    "                imstack = shifted_obs\n",
    "            #If this is not the first iteration, stack it with the others using vstack\n",
    "            else:\n",
    "                imstack = np.vstack((imstack, shifted_obs))\n",
    "        #Create a combination of the images by taking the median pixel value of the observations\n",
    "        #This adds some accounting for outliers without requireing a complex analysis of the variables\n",
    "        #that can contribute to differences between observations\n",
    "        raw_smoothed_image = np.median(imstack, axis = 0)\n",
    "        #Create a new figure\n",
    "        fig, ax = plt.subplots(figsize = (10,10))\n",
    "        #Plot the image with the median pixel generated by the stacked observations\n",
    "        #CHECK IN: see if division by observation number is necessary, prove that it returns propper fux vals\n",
    "        implot = plt.imshow(np.flip((raw_smoothed_image/numobs).T, axis = 0), cmap = 'pink', interpolation = None, origin = 'upper', \n",
    "                            extent = [np.min(x_axis), np.max(x_axis), np.min(y_axis), np.max(y_axis)]) \n",
    "        #Add a colorbar\n",
    "        plt.colorbar()\n",
    "        #Loop through the point-like structure pulled out in each individual observation, plot each\n",
    "        #observation in a different color and indicate this in the legend\n",
    "        for scat in range (len(upper_obs_data)):\n",
    "            plt.scatter(upper_obs_data[scat]['ra'], upper_obs_data[scat]['dec'], s = upper_obs_data[scat]['mag'], \n",
    "                        label = f'Observation {scat}')\n",
    "        #Display legend\n",
    "        plt.legend()\n",
    "        #Invert RA values\n",
    "        plt.gca().invert_xaxis()\n",
    "        plt.title(f'Median Observation and Point-like structure from individual observations')\n",
    "        #Add x and y labels\n",
    "        plt.xlabel('RA (Deg)')\n",
    "        plt.ylabel('DEC (Deg)')\n",
    "        #Save the figure to the current working directory and create a unique filename based on the\n",
    "        #index identifyer, clustering radius, and flux percentage cut-off\n",
    "        plt.savefig(f'source{index}medianobsradius{radius}bright{percent_total}scatternoclust.png')\n",
    "        #Display the figure\n",
    "        plt.show()\n",
    "        #MASTER CLUSTERING PULLED POINTS\n",
    "        #Create empty lists to store the data\n",
    "        mastr_ras = []\n",
    "        mastr_decs = []\n",
    "        mastr_mags = []\n",
    "        #Loop through the pulled point-like structure for the observations\n",
    "        for k in range(0, len(upper_obs_data)):\n",
    "            #Identify the RA, DEC, and flux values for these points, put them all together into\n",
    "            #a master list\n",
    "            mastr_ras.extend(upper_obs_data[k]['ra'])\n",
    "            mastr_decs.extend(upper_obs_data[k]['dec'])\n",
    "            mastr_mags.extend(upper_obs_data[k]['mag'])\n",
    "        #Define a radius and flux percentage cutoff for this clustering\n",
    "        radius = 0.000277778*30\n",
    "        percent_total = .15\n",
    "        #Call the clust_points function to cluster the point-like structure from all observations\n",
    "        clustdict = clust_points(mastr_ras, mastr_decs, mastr_mags, np.mean(cp), radius, percent_total)\n",
    "        #Create empty lists to store information about what points need to be removed\n",
    "        true_keys = []\n",
    "        false_keys = []\n",
    "        #Define an empty dictionary to store information about the clustered points\n",
    "        obscontriblen = {}\n",
    "        #Try catches all cases except those where no points were clustered\n",
    "        try:\n",
    "            #Loop through the components within the dictionary. Only one observation key is generated\n",
    "            #as the points were fed into the function as if they were a single observation\n",
    "            for key in clustdict[0]['obs0'].keys():\n",
    "                #Defing an empty list to store the infomation for this component\n",
    "                contribobsclust = []\n",
    "                #Loop through the obseration data\n",
    "                for o in range(0, len(upper_obs_data)):\n",
    "                    #Pull out the RA and DEC for this observation\n",
    "                    observation_ra = upper_obs_data[o]['ra']\n",
    "                    observation_dec = upper_obs_data[o]['dec']\n",
    "                    #Loop through the ammount of individual components identified in this obesrvation\n",
    "                    for indv in range(len(clustdict[0]['obs0'][key]['component_ras'])):\n",
    "                        #Check if the component is in the untouched RA\n",
    "                        if clustdict[0]['obs0'][key]['component_ras'][indv] in observation_ra:\n",
    "                            #Check if the component is also in the untouched DEC\n",
    "                            if clustdict[0]['obs0'][key]['component_decs'][indv] in observation_dec:\n",
    "                                #if so, append the integer loop number\n",
    "                                contribobsclust.append(o)\n",
    "                #If the point has contributions from more than half of the observations of the source\n",
    "                #it is kept by adding the key to the \"true\" key list. If not, the key is added to the \n",
    "                #false list\n",
    "                if len(set(contribobsclust)) > (len(upper_obs_data) * 0.5):\n",
    "                    true_keys.append(key)\n",
    "                    obscontriblen[key] = len(set(contribobsclust))\n",
    "                else:\n",
    "                    false_keys.append(key)\n",
    "                    obscontriblen[key] = len(set(contribobsclust))\n",
    "        #Except catches cases where the observation has no contributing clustered points\n",
    "        except:\n",
    "            contribobsclust = []\n",
    "        #This next step in the code identifies points that were pulled out of their original observations\n",
    "        #but were clustered into a point that did not pass the checks to be put into the final model.\n",
    "        #These points must be thrown back into the models for their original observations so that they\n",
    "        #are still considered in the final model, just not as part of a point source.\n",
    "        #Create empty lists to store the data for the points that need to be thrown back\n",
    "        backras = []\n",
    "        backdecs = []\n",
    "        backmags = []\n",
    "        #Loop through the observations \n",
    "        for obz in range(len(upper_obs_data)):\n",
    "            #Create empty lists to store the information for this observation\n",
    "            obsbackra = []\n",
    "            obsbackdec = []\n",
    "            obsbackmag = []\n",
    "            #Create an integer that can be increased to track iterations\n",
    "            numbertest = 0\n",
    "            #Loop through the clustered points in this observaiton\n",
    "            for key in pointlike_dict[f'obs{obz}'].keys():\n",
    "                #Try here catches cases where multiple clustered points were identified for the observation\n",
    "                try:\n",
    "                    #Loop through the points\n",
    "                    for upperpoint in range(0, len(pointlike_dict[f'obs{obz}'][key]['weight_ra'])):\n",
    "                            #Grab the weighted RA and DEC positions from the dictionary\n",
    "                            tstra = pointlike_dict[f'obs{obz}'][key]['weight_ra'][upperpoint]\n",
    "                            tstdec = pointlike_dict[f'obs{obz}'][key]['weight_dec'][upperpoint]\n",
    "                            #Loop through the keys identified as belonging to points that do not pass the \n",
    "                            #checks and need to be thrown back\n",
    "                            for k in false_keys:\n",
    "                                #Get the RA and DECs of the points that contributed to the clustered points\n",
    "                                #for the key we are iterating over\n",
    "                                cras = clustdict[0]['obs0'][k]['component_ras']\n",
    "                                cdecs = clustdict[0]['obs0'][k]['component_decs'] \n",
    "                                #Check if the RA and DEC appear in the cluster\n",
    "                                if tstra in cras:\n",
    "                                    if tstdec in cdecs:\n",
    "                                        #If they do, extend the original dictionary keys to include the \n",
    "                                        #previously tossed out points\n",
    "                                        numbertest+=1\n",
    "                                        obsbackra.extend(pointlike_dict[f'obs{obz}'][key]['component_ras'])\n",
    "                                        obsbackdec.extend(pointlike_dict[f'obs{obz}'][key]['component_decs'])\n",
    "                                        obsbackmag.extend(pointlike_dict[f'obs{obz}'][key]['component_mags'])\n",
    "                #Except catches all cases where singular or no points were captured in the clustering\n",
    "                except:\n",
    "                    #Set the RA and DEC coordinates for the single point if it exists\n",
    "                    #upperpoint = 0\n",
    "                    tstra = pointlike_dict[f'obs{obz}'][key]['weight_ra']\n",
    "                    tstdec = pointlike_dict[f'obs{obz}'][key]['weight_dec']\n",
    "                    #Loop through the keys for points that need to be thrown back and check if\n",
    "                    #any match this coordinate\n",
    "                    for k in false_keys:\n",
    "                        cras = clustdict[0]['obs0'][k]['component_ras']\n",
    "                        cdecs = clustdict[0]['obs0'][k]['component_decs'] \n",
    "                        if tstra in cras:\n",
    "                            if tstdec in cdecs:\n",
    "                                #Increase the iteration tracker and extend the original observation arrays\n",
    "                                #if they pass the checks\n",
    "                                numbertest+=1\n",
    "                                obsbackra.extend(pointlike_dict[f'obs{obz}'][key]['component_ras'])\n",
    "                                obsbackdec.extend(pointlike_dict[f'obs{obz}'][key]['component_decs'])\n",
    "                                obsbackmag.extend(pointlike_dict[f'obs{obz}'][key]['component_mags'])\n",
    "            #Check if there are points that need to be thrown back into the original observations\n",
    "            if np.sum(numbertest) != 0:\n",
    "                #Throw these data points into the apropriate arrays\n",
    "                backras.append(obsbackra)\n",
    "                backdecs.append(obsbackdec)\n",
    "                backmags.append(obsbackmag)\n",
    "            else:\n",
    "                #Add 0s if the observation has nothing to be thrown back\n",
    "                backras.append(0)\n",
    "                backdecs.append(0)\n",
    "                backmags.append(0)\n",
    "        #Create 2 iteration trackers and create an empty list to store the new observation data        \n",
    "        obs_data = []\n",
    "        n = -1\n",
    "        numobs = 0\n",
    "        #Loop through the observations in the dictionary\n",
    "        for dat in range(len(pointlike_dict)):\n",
    "            #Create empty lists to store the modified observation data\n",
    "            upper_ra_points = []\n",
    "            upper_dec_points = []\n",
    "            upper_mag_points = []\n",
    "            contribra = []\n",
    "            contribdec = []\n",
    "            contribmag = []\n",
    "            imgras = []\n",
    "            imgdecs = []\n",
    "            imgmags = []\n",
    "            #Increase the iteration tracker\n",
    "            n = n+1\n",
    "            match_idx = []\n",
    "            #Loop through the clustered points in the observations\n",
    "            for key in pointlike_dict[f'obs{n}'].keys():\n",
    "                #Add the data from the components to the master lists\n",
    "                upper_ra_points.append(pointlike_dict[f'obs{n}'][key]['weight_ra'])\n",
    "                upper_dec_points.append(pointlike_dict[f'obs{n}'][key]['weight_dec'])\n",
    "                upper_mag_points.append(pointlike_dict[f'obs{n}'][key]['sum_match'])\n",
    "                contribra.extend(pointlike_dict[f'obs{n}'][key]['component_ras'])\n",
    "                contribdec.extend(pointlike_dict[f'obs{n}'][key]['component_decs'])\n",
    "                contribmag.extend(pointlike_dict[f'obs{n}'][key]['component_mags'])\n",
    "            #Try catches cases where there are points that need to be added to original\n",
    "            #observations and extends the observation data apropriately\n",
    "            try:\n",
    "                if np.sum(backras[dat]) != 0:\n",
    "                    imgras.extend(backras[dat])\n",
    "                    imgdecs.extend(backdecs[dat])\n",
    "                    imgmags.extend(backmags[dat])\n",
    "            #Except catches cases where there is nothing to extend and adds a 0 to indicate nothing\n",
    "            #was thrown back from these observations\n",
    "            except:\n",
    "                imgras.append(0)\n",
    "                imgdecs.append(0)\n",
    "                imgmags.append(0)\n",
    "            #Try catches cases where there exists data that needs to be \"smoothed\" and the \n",
    "            #observation is not point-like\n",
    "            try:\n",
    "                #Loop through the coordinates in the RA and DEC coordinates\n",
    "                for coord in range(len(untouched_ra[dat])):\n",
    "                    #Check if the coordinate is already in the matrix\n",
    "                    #CHECK delete and: if error\n",
    "                    if (untouched_ra[dat][coord] not in np.array(contribra)) and (untouched_dec[dat][coord] not in np.array(contribdec)):\n",
    "                        #If so, add the coordinates\n",
    "                        imgras.append(untouched_ra[dat][coord])\n",
    "                        imgdecs.append(untouched_dec[dat][coord])\n",
    "                        imgmags.append(untouched_mag[dat][coord])\n",
    "                #Increase the iteration tracker and create an empty data matrix for the observation\n",
    "                numobs += 1\n",
    "                data = np.matrix(np.zeros((100,100)))\n",
    "                #Loop through the list to get the RA and DEC positional values and the flux value\n",
    "                for val in range (0, len(imgdecs)):\n",
    "                        smooth_x = imgras[val]\n",
    "                        smooth_y = imgdecs[val]\n",
    "                        smooth_m = imgmags[val]\n",
    "                        #Enumerate the x and y axes, loop through these to calculate the values for the \n",
    "                        #gaussian spreading across the data matrix\n",
    "                        for xind, xval in enumerate(x_axis):\n",
    "                            for yind, yval in enumerate(y_axis):\n",
    "                                data[xind, yind] += gaussian(1.5*(x_axis[1]-x_axis[0]), smooth_x, smooth_y, xval, yval, smooth_m)\n",
    "            #Except catches cases where the observation in point-like and the data matrix should have no vlaues\n",
    "            except:\n",
    "                data = np.matrix(np.zeros((100,100)))\n",
    "            #Add the data matrix to the list of diffuse observation components\n",
    "            obs_data.append(data)\n",
    "            #Initialize a new figure\n",
    "            fig, ax = plt.subplots(figsize = (10,10))\n",
    "            #Plot the data matrix for this observation with the necessary components thrown back\n",
    "            implot = plt.imshow(np.flip(data.T, axis = 0), cmap = 'pink', interpolation = None, origin = 'upper', \n",
    "                               extent = [np.min(x_axis), np.max(x_axis), np.min(y_axis), np.max(y_axis)])\n",
    "            #Add x and y labels\n",
    "            plt.xlabel('RA (Deg)')\n",
    "            plt.ylabel('DEC (Deg)')\n",
    "            #Display colorbar scale\n",
    "            plt.colorbar()\n",
    "            #Invert the RA axis\n",
    "            plt.gca().invert_xaxis()\n",
    "            #Add a title to the plot\n",
    "            plt.title(f'Observation {dat}, Second Iteration With Points Returned')\n",
    "            #Save the figure with a unique identifier\n",
    "            plt.savefig(f'source{index}observation{dat}radius{radius}bright{percent_total}diffusereturned.png')\n",
    "            plt.show()\n",
    "            plt.figure()\n",
    "        #MAKE FINAL PLOT\n",
    "        #Grab the original data from the DataFrame\n",
    "        mags = table.loc[index][6::9]#upper level magnitude values\n",
    "        ra_center = table.loc[index][8::9]#upper level ra values\n",
    "        dec_center = table.loc[index][9::9]#upper level dec values\n",
    "        ext_ras = table.loc[index][10::9]\n",
    "        ext_decs = table.loc[index][11::9]\n",
    "        #Find dimension of one square pixel\n",
    "        pixel_dim = x_axis[1]-x_axis[0]\n",
    "        #SHIFTING THE OBSERVAITONS\n",
    "        #Create empty lists for shifting the observations\n",
    "        shifts = []\n",
    "        newdata = []\n",
    "        #Create empty lists to store the pulled clustered points\n",
    "        truepulled_ra = []\n",
    "        truepulled_dec = []\n",
    "        truepulled_mag = []\n",
    "        #Create a None variable to stack the observations\n",
    "        imstack = None    \n",
    "        #Loop through the data matricies\n",
    "        for img in range(len(obs_data)):\n",
    "            #Expand the dimensions of the matrix\n",
    "            shifted_obs = np.expand_dims(obs_data[img], axis = 0)\n",
    "            #Check if this is the first observation\n",
    "            if imstack is None:\n",
    "                #If it is, use it to initialize the data stack\n",
    "                imstack = shifted_obs\n",
    "            else:\n",
    "                #If it is not, add it to the image stack\n",
    "                imstack = np.vstack((imstack, shifted_obs))\n",
    "        #Create a smoothed image by taking the median value for all positions in the\n",
    "        #imstack variable\n",
    "        raw_smoothed_image = np.median(imstack, axis = 0)\n",
    "        #Create a new figure\n",
    "        fig, ax = plt.subplots(figsize = (10,10))\n",
    "        #Divide the pixel values by the number of observations\n",
    "        #CHECK that this produces apropriate flux values\n",
    "        displaydata = np.flip((raw_smoothed_image/numobs).T, axis = 0)\n",
    "        #Plot the final data matrix generated by the combined observations\n",
    "        implot = plt.imshow(displaydata, cmap = 'pink', interpolation = None, origin = 'upper', \n",
    "                            extent = [np.min(x_axis), np.max(x_axis), np.min(y_axis), np.max(y_axis)]) \n",
    "        plt.colorbar()\n",
    "        #Loop through the clustered points identified for this observation and grab the weighter RA and \n",
    "        #DEC positions. Divide the combined flux observed by the total number of observations\n",
    "        #CHECK should this be total observations, or total for observations that contained this point\n",
    "        for k in true_keys:\n",
    "            truepulled_ra.append(clustdict[0]['obs0'][k]['weight_ra'])\n",
    "            truepulled_dec.append(clustdict[0]['obs0'][k]['weight_dec'])\n",
    "            numberobs = obscontriblen[k]\n",
    "            truepulled_mag.append(np.sum(clustdict[0]['obs0'][k]['component_mags'])/numberobs)\n",
    "        #Create a scatter plot on top of the plotted data matrix to display the final point-like structure\n",
    "        for point in range(0, len(truepulled_ra)):\n",
    "            plt.scatter(truepulled_ra[point], truepulled_dec[point], \n",
    "                        label = f'Point Like Structure, {truepulled_mag[point]:.4f} Jy')\n",
    "        #Display the legend\n",
    "        plt.legend()\n",
    "        #Invert the RA axis\n",
    "        plt.gca().invert_xaxis()\n",
    "        #Create x and y axis labels\n",
    "        plt.xlabel('RA (Deg)')\n",
    "        plt.ylabel('DEC (Deg)')\n",
    "        #Create a title for the final plot\n",
    "        plt.title(f'Final Model for Source {index} With Point Like Structure')\n",
    "        #Save the figure with a unique name and display it\n",
    "        plt.savefig(f'source{index}medianobsradius{radius}bright{percent_total}falsereturned.png')\n",
    "        plt.show()\n",
    "        return truepulled_ra, truepulled_dec, truepulled_mag, displaydata, x_axis, y_axis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "501166a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_SkyModel(table, source_ids, is_big, sav_path, source_freq):\n",
    "    \"\"\"\n",
    "    table: pandas DataFrame generated by the match_to_GLEAM function\n",
    "    \n",
    "    source_ident: List of source identifyers or list single source identifyer to be replaced in\n",
    "                  the generated catalog. This list can be generated by the source_search function.\n",
    "    \n",
    "    is_big: list of length matching source_ident containing a 0 for every source that is not \n",
    "            too large to fit in the modeling area and a 1 for every source that is too large\n",
    "            for the modeling area. This list can be generated by the source_search function.\n",
    "            \n",
    "    sav_path: absolute path to the folder where the files should be saved\n",
    "    \n",
    "    source_freq: an array of frequency values with units like Hz of length matching that of source_ident.\n",
    "                For example, if the frequency of the observations is 1.842e8 Hz, the input should look\n",
    "                like the below:\n",
    "                source_freq = np.repeat(1.824e8*u.Hz, len(source_ident))\n",
    "    Returns\n",
    "    ---\n",
    "    For a more complete descripion of the plots and print statements generated as this function runs, see\n",
    "    the doccumentation for the create_plots function\n",
    "    \n",
    "    Upon completion, the function will print: Modeling has completed. A SkyModel has been saved to \n",
    "    savpath + Combined_models.txt\n",
    "    \n",
    "    Combined_models.txt is a SkyModel object. This SkyModel contains the models for the source_ident sources\n",
    "    and the original models from table. Sources in source_ident and in the surrounding area modeled by those\n",
    "    sources have been removed from the sources in table and are not modeled\n",
    "    \n",
    "    The SkyModel has several columns.\n",
    "    \n",
    "    SOURCE_ID contains string names for each of the generated sources. Diffuse sources are broken into \n",
    "            individual sources. Each name has an original GLEAM identifier followed by an integer number.\n",
    "            If there only exists one string name with a zero following, this represents a point source.\n",
    "            If there are several names with identical strings but different integer numbers, these \n",
    "            components belong to the same source\n",
    "            \n",
    "    RA_J2000 [deg] contains decimal degree coordinates for the RA positions of the points\n",
    "    \n",
    "    DEC_J2000 [deg] contains decimal degree coordinates for the DEC positions of the points\n",
    "    \n",
    "    Flux [Jy] contains the flux values for these points \n",
    "    \n",
    "    Frequency [Hz] contains the frequency values for the points\n",
    "    \n",
    "    \"\"\"\n",
    "    #Cut is the minimum flux value in Jy a pixel must satisfy to be comsidered part of the source and\n",
    "    #added to the final catalog.\n",
    "    cut = 1e-4\n",
    "    #Generate empty lists to store the data in\n",
    "    idents = []\n",
    "    sourceras = []\n",
    "    sourcedecs = []\n",
    "    sourcestokes = []\n",
    "    sourcefs = []\n",
    "    extended_model_groups = []\n",
    "    num = -1\n",
    "    remove_inds = []\n",
    "    #Loop through the sources in the input list\n",
    "    for i in range(len(source_ids)):\n",
    "        #Generate a list of the indices in the table\n",
    "        inds = table.index.to_list()\n",
    "        #Grab the RA and DEC values for all the sources in the table\n",
    "        r= table['RA']\n",
    "        d = table['DEC']\n",
    "        #Grab the identifyer, RA, and DEC for the current source in the loop, check the value from the \n",
    "        #is_big list\n",
    "        identif = source_ids[i]\n",
    "        big = is_big[i]\n",
    "        source_r = tab.loc[identif]['RA']\n",
    "        source_d = tab.loc[identif]['DEC']\n",
    "        #If the source is not too big for the modeling area\n",
    "        if big == 0:\n",
    "            #Grab the limits on the modeling area and identify sources within the larger catalog that \n",
    "            #fit within this area\n",
    "            tab_ind = (r<(source_r +.25)) & (r>(source_r -.25))& (d<(source_d +.25)) & (d>(source_d -.25))\n",
    "            inds_arround = tab[tab_ind].index.to_list()\n",
    "            remove_inds.extend(inds_arround)\n",
    "        else:\n",
    "            #Do the same for a sightly larger area should the source be flagged as big\n",
    "            tab_ind = (r<(source_r +.35)) & (r>(source_r -.35))& (d<(source_d +.35)) & (d>(source_d -.35))\n",
    "            inds_arround = tab[tab_ind].index.to_list()\n",
    "            remove_inds.extend(inds_arround)\n",
    "        #Increase the variable that tracks iterations of this loop\n",
    "        num +=1\n",
    "        #Generate the RA, DEC, and flux arrays for the data with the x and y axes from the create_plots function\n",
    "        truepulled_ra, truepulled_dec, truepulled_mag, displaydata, x_axis, y_axis = create_plots(table, source_ids[i])\n",
    "        #Generate some empty lists to store the data for the pixel values\n",
    "        pix_mag = []\n",
    "        pix_ra = []\n",
    "        pix_dec = []\n",
    "        #Loop thorugh the enumerated x and y axes to locate the pixel values\n",
    "        for xind, xval in enumerate(x_axis):\n",
    "            for yind, yval in enumerate(y_axis):\n",
    "                #Check if the data matrix pixel value at a given location has a flux value greater than\n",
    "                #the specefied cutoff value\n",
    "                if displaydata[xind, yind] > cut:\n",
    "                    #If the pixel passes the check, append the RA and DEC coordinate values along with the \n",
    "                    #flux values to the appropriate lists\n",
    "                    pix_mag.append(displaydata[xind, yind])\n",
    "                    pix_ra.append(xval)\n",
    "                    pix_dec.append(yval)\n",
    "        #Add the RA, DEC, and flux values for the clustered points that were pulled from the data matrix to \n",
    "        #the larger pixel lists\n",
    "        pix_ra.extend(truepulled_ra)\n",
    "        pix_dec.extend(truepulled_dec)\n",
    "        pix_mag.extend(truepulled_mag)\n",
    "        #Create a list of SkyCoord objects from these RA and DEC vlaues, specifying that they are in \n",
    "        #units of decimal degrees\n",
    "        coords = SkyCoord(pix_ra*u.degree, pix_dec*u.degree)\n",
    "        ras = coords.ra\n",
    "        decs = coords.dec\n",
    "        #Identify the lenght of these coordinate lists\n",
    "        compind = len(ras)\n",
    "        #Grab the ientifyer for the source being iterated over\n",
    "        sourceid = source_ids[i]\n",
    "        #Create an empty list to store component names\n",
    "        compnames = []\n",
    "        #Generate a unique name for every point in the data arrays from the original source \n",
    "        #identifyer and the integer index of the point in the list\n",
    "        for ind in range(compind):\n",
    "            indname = sourceid + '_' + str(ind)\n",
    "            compnames.append(indname)\n",
    "        #Create an array of the same length as the pixel lists containing the value for the observation\n",
    "        #frequency specified at function call\n",
    "        source_freqs = np.repeat(source_freq[i],len(pix_ra))\n",
    "        #Create an array for the stokes parameter from the flux list\n",
    "        #IMPROVE COMMENTS FOR THIS LINE\n",
    "        stokes = np.zeros((4, 1, len(pix_ra)), dtype = float)\n",
    "        stokes[0, :, :] = pix_mag\n",
    "        #Create an extended model group by repeating the iteration tracker for the lengths of the pixel values\n",
    "        extended_model_group = np.repeat(num, len(ras))\n",
    "        #Concatenate the generated component names and component names from previous iterations\n",
    "        #into one array\n",
    "        idents = np.concatenate((idents, compnames), axis = 0)\n",
    "        #Create Longitude and Latitude arrays from the RA and DEC coordinates of the source and concatenate\n",
    "        #this with the same array from previous loops\n",
    "        sourceras = np.concatenate((Longitude(sourceras, unit = u.deg), ras), axis = 0)\n",
    "        sourcedecs = np.concatenate((Latitude(sourcedecs, unit = u.deg), decs), axis = 0)\n",
    "        #Concatenate the frequency array with the previous iterations\n",
    "        sourcefs = np.concatenate((sourcefs, source_freqs), axis = 0)\n",
    "        #Concatenate the extended model group array with the array from the previous loops\n",
    "        extended_model_groups = np.concatenate((extended_model_groups, extended_model_group), axis = 0)\n",
    "        #Check if this is the first iteration\n",
    "        if len(sourcestokes)<1:\n",
    "            #If sourcestokes has not yet experienced a concatenation, it must be initialized by simply\n",
    "            #extending soutcestoked with the stokes array\n",
    "            sourcestokes.extend(stokes) \n",
    "        else:\n",
    "            #If this is not the first iteration, concatenate stokes with the previous iteration\n",
    "            sourcestokes= np.concatenate((sourcestokes, stokes), axis = 2)\n",
    "    #Create an untouched version of the index array for the input DataFrame\n",
    "    inds = table.index.to_list()\n",
    "    #Loop through the indices and remove any names that appear in either the list of input sources\n",
    "    #or in the sources identified as existing within the modeling area\n",
    "    for name in inds:\n",
    "        if name in source_ids:\n",
    "            inds.remove(name)\n",
    "        elif name in remove_inds:\n",
    "            inds.remove(name)\n",
    "    #Check the intersection of the resulting list and original input to ensure no repeats are present\n",
    "    st = set(inds) & set(source_ids)\n",
    "    if len(st) !=0:\n",
    "        for nm in st:\n",
    "            #If the set is not empty, remove these sources from the original list\n",
    "            inds.remove(nm)\n",
    "    #Create a section of the original DataFrame tha only includes the remaining portion of the original \n",
    "    #catalog that was not extracted \n",
    "    remaining_GLEAM = table[table.index.isin(inds)]\n",
    "    #Create some empty lists to store the data and define an integer iteration tracker to increase\n",
    "    #as the function loops\n",
    "    idents_gleam = []\n",
    "    sourceras_gleam = []\n",
    "    sourcedecs_gleam = []\n",
    "    sourcestokes_gleam = []\n",
    "    sourcefs_gleam = []\n",
    "    extended_model_groups_gleam = []\n",
    "    num_gleam = -1\n",
    "    #Loop through these remaining sources\n",
    "    for source_name in remaining_GLEAM.index:\n",
    "        #Check if the original input catalog identified this source as diffuse, or containing multiple\n",
    "        #components. If it did, the source needs to be treated like the other modeled diffuse sources\n",
    "        if np.sum(tab.loc[source_name]['RA EO GLEAM']) >0:\n",
    "            #Increase the integer iteration tracker and pull the information for the diffuse\n",
    "            #components from the original DataFrame\n",
    "            num_gleam +=1\n",
    "            g_ra = tab.loc[source_name]['RA EO GLEAM']\n",
    "            g_dec = tab.loc[source_name]['DEC EO GLEAM']\n",
    "            g_mag = tab.loc[source_name]['Mag EO GLEAM']\n",
    "            #Create SkyCoord objects for these points\n",
    "            coords_g = SkyCoord(g_ra*u.degree, g_dec*u.degree)\n",
    "            ras_g = coords_g.ra\n",
    "            decs_g = coords_g.dec\n",
    "            #Check the length of the coordinates\n",
    "            compind = len(g_ra)\n",
    "            #Create empty list to store component names\n",
    "            compnames = []\n",
    "            #Generate a unique name for every point in the data arrays from the original source \n",
    "            #identifyer and the integer index of the point in the list\n",
    "            for ind in range(compind):\n",
    "                indname = source_name + '_' + str(ind)\n",
    "                compnames.append(indname)\n",
    "            #Create an array of the same length as the pixel lists containing the value for the observation\n",
    "            #frequency specified at function call\n",
    "            source_freqs_gleam = np.repeat(source_freq[0],len(g_ra))\n",
    "            #Create an array for the stokes parameter from the flux list\n",
    "            stokes_gleam = np.zeros((4, 1, len(g_ra)), dtype = float)\n",
    "            stokes_gleam[0, :, :] = g_mag\n",
    "            #Create an extended model group by repeating the iteration tracker for the lengths of the pixel values\n",
    "            extended_model_group_gleam = np.repeat(num_gleam, len(g_ra))\n",
    "            #Concatenate the generated component names and component names from previous iterations\n",
    "            #into one array\n",
    "            idents_gleam = np.concatenate((idents_gleam, compnames), axis = 0)\n",
    "            #Create Longitude and Latitude arrays from the RA and DEC coordinates of the source and concatenate\n",
    "            #this with the same array from previous loops\n",
    "            sourceras_gleam = np.concatenate((Longitude(sourceras_gleam, unit = u.deg), ras_g), axis = 0)\n",
    "            sourcedecs_gleam = np.concatenate((Latitude(sourcedecs_gleam, unit = u.deg), decs_g), axis = 0)\n",
    "            #Concatenate the frequency array with the previous iterations\n",
    "            sourcefs_gleam = np.concatenate((sourcefs_gleam, source_freqs_gleam), axis = 0)\n",
    "            #Concatenate the extended model group array with the array from the previous loops\n",
    "            extended_model_groups_gleam = np.concatenate((extended_model_groups_gleam, extended_model_group_gleam), axis = 0)\n",
    "            #Check if this is the first iteration\n",
    "            if len(sourcestokes_gleam)<1:\n",
    "                #If sourcestokes has not yet experienced a concatenation, it must be initialized by simply\n",
    "                #extending soutcestoked with the stokes array\n",
    "                sourcestokes_gleam.extend(stokes_gleam) \n",
    "            else:\n",
    "                #If this is not the first iteration, concatenate stokes with the previous iteration\n",
    "                sourcestokes_gleam= np.concatenate((sourcestokes_gleam, stokes_gleam), axis = 2)\n",
    "        #For casees where the source in the original catalog does not have diffuse components\n",
    "        else:\n",
    "            #Increase the integer that tracks the loop iterations\n",
    "            num_gleam +=1\n",
    "            #Get the RA and DEC positions and flux values from the original input DataFrame \n",
    "            g_ra = tab.loc[source_name]['RA']\n",
    "            g_dec = tab.loc[source_name]['DEC']\n",
    "            g_mag = tab.loc[source_name]['Mag GLEAM']\n",
    "            #Create SkyCoord objects for these positions\n",
    "            coords_g = SkyCoord(g_ra*u.degree, g_dec*u.degree)\n",
    "            ras_g = coords_g.ra\n",
    "            decs_g = coords_g.dec\n",
    "            #Create a singular component name for the object\n",
    "            compnames = [source_name + '_' + str(0)]\n",
    "            #Create an array of the same length as the pixel lists containing the value for the observation\n",
    "            #frequency specified at function call\n",
    "            source_freqs_gleam = np.repeat(source_freq[0],1)\n",
    "            #Create an array for the stokes parameter from the flux list\n",
    "            stokes_gleam = np.zeros((4, 1, 1), dtype = float)\n",
    "            stokes_gleam[0, :, :] = g_mag\n",
    "            #Create an extended model group by repeating the iteration tracker for the lengths of the pixel values\n",
    "            extended_model_group_gleam = np.repeat(num_gleam, 1)\n",
    "            #Concatenate the generated component names and component names from previous iterations\n",
    "            #into one array\n",
    "            idents_gleam = np.concatenate((idents_gleam, compnames), axis = 0)\n",
    "            #Create Longitude and Latitude arrays from the RA and DEC coordinates of the source and concatenate\n",
    "            #this with the same array from previous loops\n",
    "            sourceras_gleam = np.concatenate((Longitude(sourceras_gleam, unit = u.deg), [ras_g]), axis = 0)\n",
    "            sourcedecs_gleam = np.concatenate((Latitude(sourcedecs_gleam, unit = u.deg), [decs_g]), axis = 0)\n",
    "            #Concatenate the frequency array with the previous iterations\n",
    "            sourcefs_gleam = np.concatenate((sourcefs_gleam, source_freqs_gleam), axis = 0)\n",
    "            #Concatenate the extended model group array with the array from the previous loops\n",
    "            extended_model_groups_gleam = np.concatenate((extended_model_groups_gleam, extended_model_group_gleam), axis = 0)\n",
    "            #Check if this is the first iteration\n",
    "            if len(sourcestokes_gleam)<1:\n",
    "                #If sourcestokes has not yet experienced a concatenation, it must be initialized by simply\n",
    "                #extending soutcestoked with the stokes array\n",
    "                sourcestokes_gleam.extend(stokes_gleam) \n",
    "            else:\n",
    "                #If this is not the first iteration, concatenate stokes with the previous iteration\n",
    "                sourcestokes_gleam= np.concatenate((sourcestokes_gleam, stokes_gleam), axis = 2)\n",
    "    #GENERATING THE SKYMODELS\n",
    "    #SkyModel for input objects\n",
    "    skyobj = SkyModel(\n",
    "            name=idents,  # string names in a list or array\n",
    "            ra=sourceras,      # astropy Longitude array\n",
    "            dec=sourcedecs,    # astropy Latitude array\n",
    "            stokes=sourcestokes,  # astopy Quantity units like Jy, shape (4, 1, Nsrcs)\n",
    "            spectral_type=\"flat\", #string spectral type\n",
    "            reference_frequency=sourcefs, # astropy Quantity, units like Hz\n",
    "            extended_model_group=np.array(list(map(str, extended_model_groups))))  # integer array where components of an extended source all have the same value, sources with only one component should have a -1\n",
    "    #skyobj.write_text_catalog(savpath + 'skyobj_EoR0_test_new.txt')\n",
    "    #SkyModel for remaining catalog objects\n",
    "    skyobj_g = SkyModel(\n",
    "            name=idents_gleam,  # string names in a list or array\n",
    "            ra=sourceras_gleam,      # astropy Longitude array\n",
    "            dec=sourcedecs_gleam,    # astropy Latitude array\n",
    "            stokes=sourcestokes_gleam,  # astopy Quantity units like Jy, shape (4, 1, Nsrcs)\n",
    "            spectral_type=\"flat\", #string spectral type\n",
    "            reference_frequency=sourcefs_gleam, # astropy Quantity, units like Hz\n",
    "            extended_model_group=np.array(list(map(str, extended_model_groups_gleam))))  # integer array where components of an extended source all have the same value, sources with only one component should have a -1\n",
    "    #skyobj_g.write_text_catalog(savpath + 'new_gleam.txt')\n",
    "    skyobj.concat(skyobj_g)\n",
    "    skyobj.write_text_catalog(savpath + 'Combined_models.txt')\n",
    "    #print(f'Modeling has completed. A SkyModel has been saved to {savpath + 'Combined_models.txt'}')\n",
    "    return print(f'Modeling has completed. A SkyModel has been saved to {savpath}Combined_models.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a8c44aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def source_search(table, n_points = None, min_mg = None, min_o = None, ra_min= None, ra_max = None, \n",
    "                  dec_min = None, dec_max = None, exclude_already_unique = True):\n",
    "    \"\"\"\n",
    "    Searches for sources to be targeted for modeling\n",
    "    \n",
    "    Parameters\n",
    "    ---\n",
    "    table: DataFrame\n",
    "          DataFrame generated by match_to_GLEAM function\n",
    "    \n",
    "    n_points: int, optional\n",
    "                the minimum number of points in any observation of the source to flag it\n",
    "    \n",
    "    min_mg: float, optional\n",
    "            the minimum brightness in Jy a source must meet to be identified\n",
    "    \n",
    "    min_o: int, optional\n",
    "            minimum number of observations a source must meet to be identified. By default, the source\n",
    "            must also be diffuse in a minimum of half of observations to be considered for modeling\n",
    "    \n",
    "    ra_min: float, optional\n",
    "            minimum RA positional value in decimal degrees\n",
    "            \n",
    "    dec_min: float, optional\n",
    "            minimum DEC positional value in decimal degrees\n",
    "            \n",
    "    ra_max: float, optional\n",
    "            maximum RA positional value in decimal degrees\n",
    "            \n",
    "    dec_max: float, optional\n",
    "            maximum DEC positional value in decimal degrees\n",
    "            \n",
    "    exclude_already_unique: bool, optional\n",
    "                            When set to True, all identifiers that are not formated with the generic\n",
    "                            'J' at the begining of their string identified will be excluded in searches.\n",
    "                            By default, this is set to True\n",
    "            \n",
    "    Returns\n",
    "    ---\n",
    "    \n",
    "    source_idents: lsit of strings or integers\n",
    "                   list contiaining identifiers for those sources meeting the specefied criteria\n",
    "    \n",
    "    observation_numbers: list of integers\n",
    "                         list contining the integer number of observatios that contributes to the sources\n",
    "                         identified in source_idents. Has length matching source_idents\n",
    "    \n",
    "    is_big: list of integer 0s and 1s\n",
    "            list containing either a 0 for sources that do not require a larger modeling area or a 1\n",
    "            for sources that do require a larger modeling area. Has length matching source_idents\n",
    "    \n",
    "    \n",
    "    in_area: integer\n",
    "             the ammount of sources in the given area that passed all checks except for that it \n",
    "             was not considered diffuse. This can be helpful in determining what fraction of sources\n",
    "             in a given area or under certain parameters are identified as diffuse.\n",
    "    \n",
    "    \"\"\"\n",
    "    #Grab the indices of the input DataFrame\n",
    "    inds = table.index\n",
    "    if exclude_already_unique == True:\n",
    "        for i in inds:\n",
    "            if list(i)[0][0] != 'J':\n",
    "                inds.remove(i)\n",
    "    #Create empty lists for returns\n",
    "    is_big = []\n",
    "    observation_numbers  = []\n",
    "    source_idents = []\n",
    "    #Create variables to be increased during iterations\n",
    "    sourcenums = 0\n",
    "    in_area = 0\n",
    "    #Print an indication that the search is begining\n",
    "    print('Begining Search')\n",
    "    #Check if optional keywords have been filled, if not replace with default values\n",
    "    num_points = n_points if n_points else 15\n",
    "    min_mag = min_mg if min_mg else 0.01\n",
    "    min_obs = min_o if min_o else 4\n",
    "    has_position_limits = 1 if ra_min else 0\n",
    "    #For cases where the search is not concentrated on one part of the sky\n",
    "    if has_position_limits == 1:\n",
    "        #Loop through the string index values\n",
    "        for index in inds:\n",
    "            sourcenums +=1\n",
    "            extended_obs = 0\n",
    "            observation = 0\n",
    "            #Create a print statement that indicates the loop's position in the \n",
    "            #source list\n",
    "            print(f'Considering source {sourcenums} of {len(inds)}', end = \"\\r\")\n",
    "            #Get the information about the source in this iteration from the DataFrame\n",
    "            mags = table.loc[index][6::9]\n",
    "            mags_eo = table.loc[index][12::9]\n",
    "            ra_center = table.loc[index][8::9]\n",
    "            dec_center = table.loc[index][9::9]\n",
    "            ext_ras = table.loc[index][10::9]\n",
    "            ext_decs = table.loc[index][11::9]\n",
    "            ext_ras = tab.loc[index][10::9]\n",
    "            ext_decs = tab.loc[index][11::9]\n",
    "            #Loop through the observations of the source\n",
    "            for i in range(0, len(mags)):\n",
    "                #Confirm that this observatoin was identified as a match\n",
    "                if mags[i] !=0:\n",
    "                    #Increase the variable that tracks observation number\n",
    "                    observation +=1\n",
    "                    #Check if the source is above the minimum indicated flux\n",
    "                    if (mags[i] > min_mag):\n",
    "                        #Check if the ammount of points in the observation is greater than the cutoff\n",
    "                        if (len(mags_eo[i]) > num_points):\n",
    "                            #If this case is satisfied, increase the iteration tracker for \n",
    "                            #extended observations\n",
    "                            extended_obs +=1\n",
    "                            #Check if the source is already in the list of identified sources\n",
    "                            if index not in source_idents:\n",
    "                                already_in = 0\n",
    "                            #Get the values for RA and DEC positional coordinates from the DataFrame\n",
    "                            #for this source from the original input GLEAM catalog\n",
    "                            ind_ra = table.loc[index]['RA']\n",
    "                            ind_dec = table.loc[index]['DEC']\n",
    "                            #Check if the RA value is greater than 180, subtract 360 if so to \n",
    "                            #center the data at 0\n",
    "                            if ind_ra > 180:\n",
    "                                ind_ra = ind_ra - 360\n",
    "                            #Check that the RA and DEC values are within the specefied values\n",
    "                            if ra_min <ind_ra < ra_max:\n",
    "                                if dec_min<ind_dec< dec_max:\n",
    "                                    #Loop through the sources already in the list of identified sources\n",
    "                                    #to check if the source currently being iterated over overlaps with\n",
    "                                    #any of these\n",
    "                                    for source in source_idents:\n",
    "                                        #Get the RA and DEC values of the source from the original GLEAM\n",
    "                                        #input catalog\n",
    "                                        source_ra = table.loc[source]['RA']\n",
    "                                        source_dec = table.loc[source]['DEC']\n",
    "                                        #Adjust the RA coordinate so that it is also centered on 0\n",
    "                                        if source_ra > 180:\n",
    "                                            source_ra = source_ra - 360\n",
    "                                        #Check if the RA and DEC values fall within the range of the modeling\n",
    "                                        #area for a source already in the list of sources\n",
    "                                        if ((source_ra -0.25)< ind_ra <(source_ra+0.25)):\n",
    "                                            if ((source_dec -0.25)< ind_dec <(source_dec+0.25)):\n",
    "                                                #If so, increase the variable that identifies if a \n",
    "                                                #source is already modeled by 1\n",
    "                                                already_in +=1\n",
    "                                        #Check if the source spills out of the 0.5 degree by 0.5 degree\n",
    "                                        #modeling area and check if the larger modeling areas clash with\n",
    "                                        #already identified sources\n",
    "                                        if abs(np.max(ext_ras[i]) - np.min(ext_ras[i]))>.5:\n",
    "                                            if ((source_ra -0.35)< ind_ra <(source_ra+0.35)):\n",
    "                                                if ((source_dec -0.5)< ind_dec <(source_dec+0.35)):\n",
    "                                                    already_in +=1\n",
    "                                        elif abs(np.max(ext_decs[i]) - np.min(ext_decs[i]))>.5:\n",
    "                                            if ((source_ra -0.35)< ind_ra <(source_ra+0.35)):\n",
    "                                                if ((source_dec -0.35)< ind_dec <(source_dec+0.35)):\n",
    "                                                    already_in +=1\n",
    "                                    #If the source has passed these checks and is not already covered by\n",
    "                                    #another source, proceed to other checks\n",
    "                                    if already_in == 0:\n",
    "                                        #Check if the source appears in the minimum number of observations\n",
    "                                        if observation >= min_obs:\n",
    "                                            #If so, increase the tracker for the number of sources in \n",
    "                                            #the area that satisfy this requirement\n",
    "                                            in_area+=1\n",
    "                                            #Check if more than half of the observations are extended\n",
    "                                            if extended_obs > (0.5*observation):\n",
    "                                                #If all these and above are satisfied, add the index \n",
    "                                                #to the master list of identified sources and the integer\n",
    "                                                #number of observations to the tracking list\n",
    "                                                source_idents.append(index) \n",
    "                                                observation_numbers.append(observation)\n",
    "                                            #Check if the source spills out of the modeling area and add a 1\n",
    "                                            #to the is_big list if this is true\n",
    "                                            if abs(np.max(ext_decs[i]) - np.min(ext_decs[i]))>.5:\n",
    "                                                if abs(np.max(ext_ras[i]) - np.min(ext_ras[i]))>.5:\n",
    "                                                    is_big.append(1)\n",
    "                                            #If the source does not spill out of the area, add a 0\n",
    "                                            #to the is_big list\n",
    "                                            else:\n",
    "                                                is_big.append(0)\n",
    "    #For cases where there is no specification for RA and DEC position limitations                                        \n",
    "    else:\n",
    "        #Loop through the string index values\n",
    "        for index in inds:\n",
    "            sourcenums +=1\n",
    "            extended_obs = 0\n",
    "            observation = 0\n",
    "            #Create a print statement that indicates the loop's position in the \n",
    "            #source list\n",
    "            print(f'Considering source {sourcenums} of {len(inds)}', end = \"\\r\")\n",
    "            #Get the information about the source in this iteration from the DataFrame\n",
    "            mags = table.loc[index][6::9]\n",
    "            mags_eo = table.loc[index][12::9]\n",
    "            ra_center = table.loc[index][8::9]\n",
    "            dec_center = table.loc[index][9::9]\n",
    "            ext_ras = table.loc[index][10::9]\n",
    "            ext_decs = table.loc[index][11::9]\n",
    "            ext_ras = table.loc[index][10::9]\n",
    "            ext_decs = table.loc[index][11::9]\n",
    "            #Loop through the observations of the source\n",
    "            for i in range(0, len(mags)):\n",
    "                #Confirm that this observatoin was identified as a match\n",
    "                if mags[i] !=0:\n",
    "                    #Increase the variable that tracks observation number\n",
    "                    observation +=1\n",
    "                    #Check if the source is above the minimum indicated flux\n",
    "                    if (mags[i] > min_mag):\n",
    "                        #Check if the ammount of points in the observation is greater than the cutoff\n",
    "                        if (len(mags_eo[i]) > num_points):\n",
    "                            #If this case is satisfied, increase the iteration tracker for \n",
    "                            #extended observations\n",
    "                            extended_obs +=1\n",
    "                            #Check if the source is already in the list of identified sources\n",
    "                            if index not in source_idents:\n",
    "                                already_in = 0\n",
    "                            #Get the values for RA and DEC positional coordinates from the DataFrame\n",
    "                            #for this source from the original input GLEAM catalog\n",
    "                            ind_ra = table.loc[index]['RA']\n",
    "                            ind_dec = table.loc[index]['DEC']\n",
    "                            #Loop through the sources already in the list of identified sources\n",
    "                            #to check if the source currently being iterated over overlaps with\n",
    "                            #any of these\n",
    "                            for source in source_idents:\n",
    "                                #Get the RA and DEC values of the source from the original GLEAM\n",
    "                                #input catalog\n",
    "                                source_ra = table.loc[source]['RA']\n",
    "                                source_dec = table.loc[source]['DEC']\n",
    "                                #Check if the RA and DEC values fall within the range of the modeling\n",
    "                                #area for a source already in the list of sources\n",
    "                                if ((source_ra -0.25)< ind_ra <(source_ra+0.25)):\n",
    "                                    if ((source_dec -0.25)< ind_dec <(source_dec+0.25)):\n",
    "                                        #If so, increase the variable that identifies if a \n",
    "                                        #source is already modeled by 1\n",
    "                                        already_in +=1\n",
    "                                #Check if the source spills out of the 0.5 degree by 0.5 degree\n",
    "                                #modeling area and check if the larger modeling areas clash with\n",
    "                                #already identified sources\n",
    "                                if abs(np.max(ext_ras[i]) - np.min(ext_ras[i]))>.5:\n",
    "                                    if ((source_ra -0.35)< ind_ra <(source_ra+0.35)):\n",
    "                                        if ((source_dec -0.5)< ind_dec <(source_dec+0.35)):\n",
    "                                            already_in +=1\n",
    "                                elif abs(np.max(ext_decs[i]) - np.min(ext_decs[i]))>.5:\n",
    "                                    if ((source_ra -0.35)< ind_ra <(source_ra+0.35)):\n",
    "                                        if ((source_dec -0.35)< ind_dec <(source_dec+0.35)):\n",
    "                                            already_in +=1\n",
    "                            #If the source has passed these checks and is not already covered by\n",
    "                            #another source, proceed to other checks\n",
    "                            if already_in == 0:\n",
    "                                #Check if the source appears in the minimum number of observations\n",
    "                                if observation >= min_obs:\n",
    "                                    in_area +=1\n",
    "                                    #Check if more than half of the observations are extended\n",
    "                                    if extended_obs > (0.5*observation):\n",
    "                                        #If all these and above are satisfied, add the index \n",
    "                                        #to the master list of identified sources and the integer\n",
    "                                        #number of observations to the tracking list\n",
    "                                        source_idents.append(index) \n",
    "                                        observation_numbers.append(observation)\n",
    "                                    #Check if the source spills out of the modeling area and add a 1\n",
    "                                    #to the is_big list if this is true\n",
    "                                    if abs(np.max(ext_decs[i]) - np.min(ext_decs[i]))>.5:\n",
    "                                        if abs(np.max(ext_ras[i]) - np.min(ext_ras[i]))>.5:\n",
    "                                            is_big.append(1)\n",
    "                                    #If the source does not spill out of the area, add a 0\n",
    "                                    #to the is_big list\n",
    "                                    else:\n",
    "                                        is_big.append(0)\n",
    "\n",
    "    print('Finished')\n",
    "    return source_idents, observation_numbers, is_big, in_area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d615e64",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90f3c0ef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "be78467b",
   "metadata": {},
   "source": [
    "## Investigating an issue: opening keyword"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d489be05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching file 4 of 82\r"
     ]
    }
   ],
   "source": [
    "#def obs_in_area(directory, path_to_new_directory, ramin, ramax, decmin, decmax)\n",
    "paths = glob.glob(new_sources + '*source_array.sav')\n",
    "ra_cents = []\n",
    "dec_cents = []\n",
    "fl = -1\n",
    "dats = []\n",
    "#iter_paths = paths[0:index_lim] if index_lim else paths\n",
    "for path in paths[0:5]:\n",
    "    fl +=1\n",
    "    os.system('clear')\n",
    "    print(f\"Searching file {fl} of {len(paths)}\", end = \"\\r\")\n",
    "    #Collect the data for each path\n",
    "    #n = n + 1\n",
    "    #Identify path to this data file\n",
    "    datpath = glob.glob(path)\n",
    "    #Create a dictionary to store the data\n",
    "    data = {}\n",
    "    #Open the data\n",
    "    data['data'] = [scipy.io.readsav(datpath[i], python_dict=True)\n",
    "        for i in range(len(datpath))]\n",
    "    #Create some empty dictionaries to store the data\n",
    "    eo = []\n",
    "    eo_ra = []\n",
    "    eo_dec = []\n",
    "    ps_RA = []\n",
    "    ps_DEC = []\n",
    "    i_mag = []\n",
    "    EO_imag = []\n",
    "    ps_ston = []\n",
    "    eo_ston = []\n",
    "    #Try using the 'catalog' keyword to open the data\n",
    "    try:\n",
    "        d_s = data['data'][0]['catalog']\n",
    "    #If the 'catalog' keyword does not exist, assume that the keyword is 'source_array'\n",
    "    except:\n",
    "        d_s = data['data'][0]['source_array']\n",
    "    #Loop over the sources in the data file\n",
    "    flag = 0\n",
    "    rs = []\n",
    "    dats.append(d_s)\n",
    "    for r in d_s['RA']:\n",
    "        if r >180:\n",
    "            flag +=1\n",
    "            rs.append(r-360)\n",
    "        else:\n",
    "            rs.append(r)\n",
    "   # if flag == 0:\n",
    "    ra_cents.append(np.mean(rs))\n",
    "    #else:\n",
    "     #   ra_cents.append(np.mean(rs)+360)\n",
    "    #yr = np.mean(np.mean(rs))\n",
    "\n",
    "    #if abs(np.max(d_s['RA'])-np.min(d_s['RA'])) > 100:\n",
    "       # rs = []\n",
    "       # for r in d_s['RA']:\n",
    "       #     if r >180:\n",
    "        #        rs.append(r-360)\n",
    "         #   else:\n",
    "           #     rs.append(r)\n",
    "        #if np.mean(rs) < 0:\n",
    "         #   ra_cents.append(np.mean(rs)+180)\n",
    "        #else:\n",
    "           # ra_cents.append(np.mean(rs))\n",
    "    #elif np.mean(d_s['RA']) < 0 :\n",
    "     #   ra_cents.append(np.mean(d_s['RA'])+360)\n",
    "   # else:        \n",
    "    #    ra_cents.append(np.mean(d_s['RA']))\n",
    "    dec_cents.append(np.mean(d_s['DEC']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d5af51d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
